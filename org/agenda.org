#+TITLE:Agenda - Computer Architecture
#+AUTHOR:Marcus Birkenkrahe
#+SUBTITLE:CSC 255 Agenda Fall 2025
#+STARTUP: overview hideblocks indent
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
#+PROPERTY: header-args:python :session *Python* :results output :exports both :noweb yes
#+PROPERTY: header-args:C :main yes :includes <stdio.h> :results output :exports both :noweb yes
#+PROPERTY: header-args:C++ :main yes :includes <iostream> :results output :exports both :noweb yes
* DONE Week 0: Overview

- Tools used and plans made
- Motivation to enrol in the course

* DONE Week 1: Overview: Why computer architecture, LogiSim installation

- [X] Reviews
- [X] Installation: LogiSim
- [X] Nand2Tetris projects
- [ ] *Research mixer on Tuesday, September 30 @Lyon - tinyurl.com/res-mix*
- [ ] Why Computer architecture?

** DONE Review Week 0

1. Which tools are we going to use in this course?
   #+begin_quote
   - Hardware Description Language (HDL)
   - LogiSim (digital circuit simulator)
   - GDB (GNU DeBugger) and GCC (GNU C Compiler)
   #+end_quote
2. What stands between source code and machine code?
   #+begin_quote
   Source code (in any high level language) is compiled into
   human-readable Assembly code and then linked to not human-readable
   object code for execution. (Though some compilers generate object
   code directly - Assembly is not always exposed.)
   #+end_quote
3. What is specialy about this intermediate step?
   #+begin_quote
   What's special about Assembly is that it depends on the CPU. Each
   machine has an instruction set architecture (ISA) that is
   specific. The assembly file and the object code of one CPU will not
   run on another.
   #+end_quote
4. Which programming languages are portable?
   #+begin_quote
   No language is "portable". Their implementation makes them
   portable:

   - C and C++: The source code is portable, but the compiled binary
     is not. You need a compiler for each target architecture.
   - Python and JavaScript: Interpreted (or Just-In-Time compiled) -
     source runs cross-platform if the interpreter is available.
   - Java: uses intermediate bytecode that runs on a Java Virtual
     Machine (JVM), which is ported to many hardware platforms.
   #+end_quote
5. What is the Gestalt principle, and how does it apply to computer
   architecture?
   #+begin_quote
   The Gestalt (German for 'shape') principle comes from psychology:
   "The whole is greater than the sum of its parts." It means that we
   perceive systems as unified structures, not just isolated
   components.

   In computer architecture, the Gestalt idea applies because a
   computer is not just CPU + memory + I/O in isolation. The
   organization and interaction of these components give rise to
   capabilities (like parallelism, pipelining, or memory hierarchies)
   that cannot be understood by looking at the parts alone.
   #+end_quote

** DONE Syllabus

- New attendance policy and app (QR code)
- Updated AI guidelines
- Project-centered grading
- Textbooks (optional): CS:APP / Nand2Tetris

** DONE Schedule

| Week(s) | Topic                           | Notes                             |
|---------+---------------------------------+-----------------------------------|
| 1       | Intro to Systems & Architecture | Abstractions, tools, setup        |
| 2–3     | Project 1: Boolean Logic        | HDL, logic gates, test scripts    |
| 4–5     | Project 2: Combinational Chips  | Adders, ALU parts, multiplexers   |
| 6–7     | Project 3: Sequential Chips     | Flip-flops, RAM, memory hierarchy |
| 8–9     | Project 4: Machine Language     | Hack code, write simple programs  |
| 10–11   | Project 5: Hack CPU             | Build CPU, connect components     |
| 12–13   | Project 6: Assembler            | Write assembler, resolve symbols  |
| 14      | Integration & Demo Prep         | I/O mapping, final integration    |
| 15      | Hack Computer Demo              | Demo: HDL → CPU → Assembler stack |

** DONE Projects

*** The Idea of Nand2Tetris
#+attr_html: :width 400px :float nil:
[[../img/michelangelo.png]]

*** The course projects (Hack hardware)
#+attr_html: :width 600px :float nil:
#+Caption: Nand2Tetris projects we may be able to complete in CSC 255.
[[../img/projects.png]]

*** Other projects (Hack software)
#+attr_html: :width 600px :float nil:
#+Caption: Nand2Tetris projects that we will not cover in CSC 255.
[[../img/projectsOS.png]]

** DONE Install LogiSim

- [[https://cdn.hackaday.io/files/1814287762215552/logisim_tutorial.pdf][The Guide to Being a LogiSim User (tutorial)]]

- Go to [[https://github.com/logisim-evolution/][github.com/logisim-evolution/]]

- Navigate to the [[https://github.com/logisim-evolution/logisim-evolution/releases][releases repo]]

- Download what you need for Windows (msi), MacOS (dmg), or Linux
  (rpm)

- You must have Java installed (in a terminal, run =java --version=)

- On Windows, double-click the .msi file and change the location to
  your local disk (=C:/Users/yourname=). Or download and run the .jar
  file.

- On MacOS, double-click the .dmg file then drag the Logisim-Evolution
  icon into the Applications folder.

- On Linux, if you're on Debian, download the .deb file and run
  #+begin_example
  sudo dpkg -i logisim-evolution-3.9.0_[cpu].deb
  #+end_example

- Check that your program exists and open it to create your first
  diagram.


** DONE Research mixer - why you should do this
#+attr_html: :width 400px :float nil:
[[../img/research_mixer.png]]

- Doing research is a great excuse to build a relationship with a
  business: People love to help, especially techies!

- This is practical networking that can help you get an internship or
  a job: You can put it on your resume, too!

- Through research, you learn to update your knowledge, and you may
  find out about interesting applications that define your career!

- Science is at crossroads for multiple reasons: It's too bloated,
  it's too expensive, it's getting too difficult, and there's AI.

** DONE Brilliant course
#+attr_html: :width 400px :float nil:
[[../img/brilliant2.png]]

- Start the free [[https://brilliant.org/courses/digital-circuits/]["digital circuits" course on brilliant.org]] to
  revitalize your digital logics knowledge and to get a view of all
  the different aspect of the CPU.

- I am on day 56 currently, starting with Nand gates - which we're
  assuming as prerequisite knowledge here.

- Includes 3 daily practice questions. You can finish in 1 month.

** DONE Review (Monday)

1. What is a compiler flag - can you give an example?
   #+begin_src bash :results output :exports both
     cd ../src
     echo -e "#include <iostream>\nint main(){\
     std::cout<<\"Hello, world!\";return 0;}" > hello.cpp
     cat hello.cpp
     g++ hello.cpp -o hello
     ./hello
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main(){std::cout<<"Hello, world!";return 0;}
   : Hello, world!

2. What does a digital NOT circuit consist of?
   #+begin_quote
   1) An input pin (Voltage ON/OFF)
   2) An output pin (Voltage OFF/ON)
   3) A Not gate

   [[../img/Not.png]]
   #+end_quote

3. What kind of application is LogiSim Evolution?
   #+begin_quote
   - Free open source software (FOSS)
   - Java application (.jar)
   - Digital Circuit simulation software
   #+end_quote

4. Discuss this with your neighbor and come to a presentable
   conclusion: Do logical statements, truth tables, and circuit
   diagrams express *exactly* the same information? For example,
   consider NOT:

   - Logical statement: =NOT(A)= or algebraically y = \not x

   - Truth table:
     | A | NOT(A) |
     |---+--------|
     | 0 |      1 |
     | 1 |      0 |

   - Digital circuit diagram:
     #+attr_html: :width 400px :float nil:
     [[../img/Not.png]]

   - Answer:
     #+begin_quote
     Yes: Logical statement, truth table and circuit diagram all show a
     representation of the same Boolean function f:x^{1}{0,1}->{0,1}.

     And No: The level and kind of of abstraction differs: Logic
     notation show algebraic manipulation; truth tables show the state
     space; and circuits show physical realization (voltage, wires,
     gates).

     If the level and kind of abstraction differs, then different
     details have been eliminated and the result has a different
     information content.
     #+end_quote

5. Which of these representations is your personal favorite? Why do
   you think that is?
   #+begin_quote
   - Some properties like identities and laws (De Morgan,
     distribution) are only visible algebraically. Expressions can
     be transformed using these laws. More information for the
     *mathematician* and fan of symbolic manipulation.
   - The truth table completely describes the function for a fixed
     number of inputs, listing all outputs. Its abstraction is close
     to the capabilities of a brute force machine (loop over all
     values). More information for the *computer scientist*.
   - Some properties (timing, propagation delay, power use) are only
     visible in the circuit-level representation: More information
     for the *engineer* and spatial thinker.
   #+end_quote

** IN PROGRESS Overview

- [X] Some questions to begin with
- [ ] Why Computer Architecture
- [ ] Arithmetic
- [ ] Assembly
- [ ] Memory
- [ ] Optimization
- [ ] Networks

** Review (Wednesday)

1. Can you write and run a C++ "hello world" pgm on the command-line?
   #+begin_src bash :results output :exports both
     # write text to stdout and redirect it into a C++ file
     echo -e "#include <iostream>\nint main() {\n  std::cout<<\"hello world\";\n}" > hello.cpp
     cat hello.cpp
     # compile and run C++ file
     g++ hello.cpp -o hello
     ./hello
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main() {
   :   std::cout<<"hello world";
   : }
   : hello world

2. What are the three parts of a computer *system*?
   #+begin_quote
   1) Hardware (CPU),
   2) system software (OS),
   3) application programs (compiler)
   #+end_quote

3. What are the parts of a computer architecture?
   #+begin_quote
   1) ISA (instruction set architecture/CPU),
   2) microarchitecture (data flow/ALU/cache),
   3) system organization (memory bus).
   #+end_quote

** NEXT Assignments for next week

- [X] [[https://lyon.instructure.com/courses/3673/assignments/50009/edit?quiz_lti][Test 1 in Canvas]] (open book) - by September 7 (Friday)

  This first test covers the material seen and taught until Friday,
  August 29. Check the reviews in the agenda.org file to prepare.


- [X] Watch "[[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][How are transistors manufactured?]]" (2024) - 30 min

  This video explains how modern microchips, containing billions of
  nanoscopic (distances of 1 billionth of a meter) transistors, are
  manufactured in semiconductor fabrication plants. Though we're not
  primarily interested in manufacturing chips, this is both
  interesting and relevant to appreciate the complexity of computer
  systems. [Review & Test].

- [X] Watch "[[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Making logic gates from transistors]]" (2015) - 15 min

  The video introduces transistors and shows how they can be used as
  building blocks for digital logic. Though this concerns physics and
  electrical engineering, seeing how logic gates are implemented via
  transistors and circuits will add to your understanding of the
  microarchitecture of computer systems. [Review & Test].

- [X] [[https://lyon.instructure.com/courses/3673/assignments/50014][Install LogiSim on your home desktop or laptop:]]

  1) Install LogiSim on your own computer.
  2) Build a NOT gate as seen in class.
  3) Take a screenshot and upload it to Canvas.

* DONE Week 2: Microchips, logic gates, (Sep 5)

- [X] Popquiz! What do you remember from the videos? Take the
  solutions home, grade yourself, and return the test next week.

- [X] Finish: Assembly and the machine.

** Review: How are Microchips made? ([[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][Branch Education, YouTube 2024]])

1. What is the approximate number of steps required to manufacture a
   modern CPU chip?
   #+begin_quote
   Around 940 steps, taking about 3 months.
   #+end_quote
2. What type of transistor structure is commonly used in today’s CPUs,
   and how small are they?
   #+begin_quote
   FinFET (Fin Field-Effect) transistors, with dimensions on the order
   of tens of nanometers (e.g., 36×6×52 nm). A 3D transistor rather
   than a planar design leading to better control of the current.
   #+end_quote
3. What are the six main categories of fabrication tools used in a
   semiconductor fab?
   #+begin_quote
   Mask-making, deposition, etching/planarization, ion implantation,
   cleaning, and metrology/inspection.

   In the video: Mask layer, adding, removing, modifying, cleaning and
   inspecting  material.
   #+end_quote
4. How does chip “binning” affect the product lines (e.g., i9, i7,
   i5)?
   #+begin_quote
   Chips with defects are categorized based on functional cores and
   features, sold under different product tiers.
   #+end_quote
5. Why is photolithography considered one of the most important steps
   in chip fabrication?
   #+begin_quote
   It transfers nanoscopic circuit patterns from photomasks onto
   wafers, enabling billions of identical transistors and wires.
   #+end_quote

** Summary: How are Microchips made? ([[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][Branch Education, YouTube 2024]])

The video explains how modern microchips, containing billions of
nanoscopic transistors, are manufactured in semiconductor fabrication
plants:
- Scale & Complexity: A CPU may hold 26 billion transistors across 80
  layers of metal interconnects, manufactured in cleanrooms the size
  of eight football fields, using machines costing up to $170M.
- Transistor Structures: Modern CPUs use FinFET transistors, only tens
  of nanometers in size, smaller than dust particles or mitochondria.
- Manufacturing Analogy: Building a chip is like baking an 80-layer
  cake with 940 steps — requiring precision at every stage or the
  product fails.
- Core Process Steps: Each layer is built by depositing insulators,
  applying light-sensitive photoresist, using photolithography with UV
  light and masks, etching away unwanted areas, depositing copper, and
  leveling the wafer with chemical mechanical planarization
  (CMP). These steps are repeated layer by layer, with frequent
  cleaning and inspection.
- Fabrication Plant Tools: Six categories of tools are used:
  1. Mask-making (photoresist, lithography, stripping)
  2. Deposition (adding metals, oxides, silicon)
  3. Etching & planarization
  4. Ion implantation (doping regions for transistors)
  5. Wafer cleaning
  6. Metrology/inspection
- Throughput & Cost: A fab may hold 435 tools and produce 50,000
  wafers monthly. Each wafer, costing ~$100, becomes worth ~$100,000
  once populated with CPUs.
- Post-Fab Steps: Chips are tested and “binned” (e.g., i9, i7, i5)
  depending on defects, cut from wafers, mounted on packages, fitted
  with heat spreaders, and tested again before sale.
- Broader Context: Microchip fabrication is secretive and
  technologically advanced, requiring immense time and resources. The
  video notes future plans for deep dives on transistors, GPUs, and
  CPU architectures.

** Review: Making logic gates from transistors ([[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Ben Eater, 2015]])

1. What are the three terminals of a transistor and what do they
   represent?
   #+begin_quote
   Emitter, Base, Collector. The base controls current flow between
   collector and emitter.

   #+attr_html: :width 400px :float nil:
   [[../img/transistor.png]]
   #+end_quote
2. What happens in the LED circuit when current flows from the base to
   the emitter?
   #+begin_quote
   The transistor switches on and allows a larger current from collector
   to emitter, lighting the LED.

   #+attr_html: :width 300px :float nil:
   [[../img/led.png]]
   #+end_quote
3. Which logic gate does a single transistor implement when it inverts the input?
   #+begin_quote
   A NOT gate (inverter): input ON → output OFF, input OFF → output ON.

   #+attr_html: :width 300px :float nil:
   [[../img/inverter.png]]

   #+end_quote
4. How is an AND gate built with two transistors?
   #+begin_quote
   The transistors are placed in series. Current flows and the LED turns
   on only if both are conducting (both inputs ON).

   #+attr_html: :width 300px :float nil:
   [[../img/and_circuit2.png]]

   #+end_quote
5. What is the difference between an OR gate and an XOR gate?
   #+begin_quote
   OR: output ON if one or both inputs are ON.  XOR: output ON only if
   exactly one input is ON (off when both are ON).

   #+attr_html: :width 300px :float nil:
   [[../img/or_circuit2.png]]

   #+attr_html: :width 300px :float nil:
   [[../img/xor_circuit2.png]]

   #+end_quote
** Summary: Making logic gates from transistors ([[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Ben Eater, 2015]])
#+attr_html: :width 600px :float nil:
[[../img/all_gates.png]]

The video introduces transistors and shows how they can be used as
building blocks for digital logic.

- A transistor has three terminals: *emitter*, *base*, *collector*.
- A small base-to-emitter current controls a larger collector-to-emitter current:
  the transistor acts as a *switch*.
- Example 1: Push button + transistor turns an LED on or off.
- Example 2: A transistor can act as an *inverter (NOT gate)*: input ON → LED OFF,
  input OFF → LED ON.
- Combining transistors yields logic gates:
  - *AND gate*: LED on only if both inputs are on.
  - *OR gate*: LED on if either input is on.
  - *XOR gate*: LED on if exactly one input is on (requires five transistors).
- Other gates (NAND, NOR, XNOR, Buffer) can be built by adding inversion.
- With these gates, more complex circuits can be built for arithmetic,
  memory, and eventually entire computers.

** Assembly and the machine

1. Why Assembly at all?
   #+begin_quote
   Assembly is the key to machine-level execution:
   1. Behavior of programs with bugs
   2. Tuning program performance
   3. Implementing system software
   4. Creating/fighting malware
   #+end_quote

2. How can you generate an Assembly file from a C file =hello.c=?
   #+begin_example
   g++ -S hello.c -o hello.s  # output = Assembly
   #+end_example

3. How can you look at the Assembly file?
   #+begin_quote
   With any text editor, or with ~cat~ on the command-line.
   #+end_quote

4. How must you compile to debug your file =segfault.c=?
   #+begin_example
   g++ -g segfault.c -o segfault  # output = Ready for gdb
   #+end_example

5. How can you debug the object code =segfault= with ~gdb~?
   #+begin_example
   gdb segfault  # import segfault and run it in gdb
   #+end_example

* DONE Week 3: Memory, Compiler chain (Sep 8, 10, 12)

- [X] Guest speaker on Wednesday 10 Sept 1 pm in Derby 255
- [X] Memory layout errors
- [X] Code optimization
- [X] Network dependency

- [X] The compiler chain
- [ ] Hardware organization
- [ ] Running the =hello world= executable

- [ ] The OS and its abstractions
- [ ] The memory organization
- [ ] Amdahl's Law
- [ ] Networks
- [ ] Concurrency and parallelism

** Review: Memory (Monday)

1. What's an architecture problem related to memory?
   #+begin_quote
   Memory: Out-of-bounds access can corrupt nearby data because memory
   must be explicitly allocated and managed.
   #+end_quote
2. What's an architecture problem related to performance?
   #+begin_quote
   Performance: Poor memory access patterns (like column-wise instead
   of row-wise) increase cache misses and slow programs.
   #+end_quote
3. What's an architecture problem related to networks?
   #+begin_quote
   Networks: Concurrency, unreliable media, and cross-platform
   differences make network programming complex despite standard
   libraries.
   #+end_quote
4. What's a socket?
   #+begin_quote
   A socket is an endpoint for communication between two programs over
   a network (including communication of a computer with itself). On
   Unix-like systems (Linux, MacOS) it's like a file descriptor - you
   can read from and write to it, but instead of accessing a file, the
   data go through the network stack.
   #+end_quote
5. What's the little/big endian problem?
   #+begin_quote
   The little/big endian problem is about how multi-byte data (like an
   ~int~ or ~double~) is stored in memory:
   - *Little-endian*: the least significant byte goes in the lowest
     memory address.
   - *Big-endian:* the most significant byte goes in the lowest memory
     address.

   The "problem" arises when data is shared between systems with
   different endianness (e.g., over a network or in files). The same
   bytes can be interpreted differently unless both sides agree on the
   byte order.

   In practice, this means that a 32-bit instruction like =0x00c58533=
   on a little endian machine will appear in memory as: =33 85 c5 00=.
   #+end_quote

** Review: Compiler chain (Wednesday)

1. What does ~make~ do? Example use?
   #+begin_quote
   ~make~ works with a configuration ~Makefile~ to compile code. An
   example is =make hello= on the command-line which uses the default
   ~Makefile~ to run =gcc hello.c -o hello= and generate an executable
   =hello= from the source file =hello.c=.
   #+end_quote

2. What is shipped alongside software source code to enable portable
   object code?
   #+begin_quote
   A build system or ~Makefile~. The compiler uses the ~Makefile~ to build
   the software for the given computer architecture.

   Portable object code = source code + Makefile/configuration.
   #+end_quote

3. Why does each character in =hello.c= have an associated number like
   104 for =h=? What does this have to do with bytes?
   #+begin_quote
   Because text is stored as bytes using ASCII encoding: each
   character is mapped to a unique integer between 0 and 255
   (2^8-1). For example, the letter =A= is ASCII =65= which in binary is
   =01000001= (2^6+1). One ASCII character = 1 byte (8 bits) in memory.
   #+end_quote

4. What does the preprocessor do in the compilation chain?
   #+begin_quote
   The preprocessor expands macros (like =#define PI 3.14=) and includes
   header files (like =#include <stdio.h>=) to create an intermediate
   (=.i=) source file.
   #+end_quote

5. In the compilation chain, what is the role of the assembler?
   #+begin_quote
   The assembler translates assembly code (=.s=) into object code
   (=.o=). With ~gdb~, you can disassemble the object machine code to see
   the assembly (as text =.s=).
   #+end_quote

   #+attr_html: :width 00px :float nil:
   #+caption: Compilation chain. Source: Bryant/O'Halloran 2016 (Fig 1.3)
   [[../img/fig1.3_compilation.png]]

* DONE Week 4: Expo, Hardware Organization, Operating System (Sep 15, 17, 19)
#+attr_html: :width 200px :float nil:
[[../img/expo.png]]

*Housekeeping:*

- [X] I made the first assignment (memory out-of-bounds demonstration)
  a little more verbose and a little simpler. If you didn't submit on
  time or you don't understand it: Ask another student, submit late
  for 50%.

- [X] Test 3 is live (25 questions).

- [ ] Two assignments are live - compiler chain and
  makefile. Unfortunately, Canvas allows only upload of one file or
  text. If you cannot fit the output on a screenshot, you need to
  submit a ZIP file.

- [X] For the ~Makefile~ assignment, just submit the results as text -
  copy of the Makefile and copy of the command-line dialogue. Like this:
  #+attr_html: :width 400px:
  [[../img/makefile.png]]

- [X] You should go to the Career Fair tomorrow. Will you?

- [X] What did you think of the Career Fair?

- [ ] Popquiz 2 (self-graded): Computer hardware organization

- [ ] Take the solution home with you. You can hand in your solution
  for bonus points if you like.

- [ ] Test 4 coming on the weekend.

*Content:*

- [X] Hardware organization
- [ ] Operating system and hardware
- [ ] Networks and hardware
- [ ] Amdahl's Law (a law about systems)
- [ ] Concurrency and parallelism

** Review: Compiler chain / Makefile

1. What kind of file does the compiler (~cc1~) produce?
   #+begin_quote
   Assembly source code (.s) for the target machine architecture.
   #+end_quote

2. What is the role of the assembler (~as~)?
   #+begin_quote
   It translates assembly code (.s) into an object file (.o) containing
   machine code instructions.
   #+end_quote

3. What does the linker (~ld~) do?
   #+begin_quote
   It combines object files (.o) and library code into a final
   executable file.
   #+end_quote

4. What is the purpose of a Makefile?
   #+begin_quote
   It automates the build process by defining rules, dependencies, and
   commands to compile programs efficiently.
   #+end_quote

5. What happens if you run =make hello= and =hello.o= is already up to
   date (i.e. the timestamp of =hello.o= is more recent than =hello.c=)?
   #+begin_quote
   Nothing is rebuilt, because ~make~ sees that the target is newer than
   its dependencies.
   #+end_quote

6. Does ~make~ only work on C programs, or also on C++ programs? And
   what about Python or R? What about Java?
   #+begin_src bash :results output
     echo -e "#include <iostream>\nint main(){std::cout<<\"hello\";return 0;}" > hello.cpp
     cat hello.cpp
     make hello  # uses the default Makefile
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main(){std::cout<<"hello";return 0;}
   : g++     hello.cpp   -o hello

   #+begin_src bash :results output
     echo -e "str(mtcars)" > mtcars.R
     cat mtcars.R
     make mtcars
     Rscript ./mtcars.R
   #+end_src

   #+RESULTS:
   #+begin_example
   str(mtcars)
   'data.frame':        32 obs. of  11 variables:
    $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
    $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
    $ disp: num  160 160 108 258 360 ...
    $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
    $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
    $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
    $ qsec: num  16.5 17 18.6 19.4 17 ...
    $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
    $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
    $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
    $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
   #+end_example

   #+begin_src bash :results output
     echo -e "print(\"hello\")" > hellopy.py
     cat hellopy.py
     make hellopy
     #python3 hellopy.py
   #+end_src

   #+RESULTS:
   : print("hello")

   #+begin_src bash :results output
     echo -e "System.out.println(\"hello\");" > helloJava.java
     cat helloJava.java
     make helloJava
   #+end_src

   #+RESULTS:
   : System.out.println("hello");

7. How must commands in a Makefile begin?
   #+begin_quote
   The =make= utility expects commands in each rule to be indented with
   a TAB character, not spaces — otherwise it throws an error.
   #+end_quote

8. How does =make clean= typically work?
   #+begin_quote
   It removes intermediate files (like .i, .s, .o, executables) so you
   can rebuild from scratch.
   #+end_quote

9. Which devices are involved in running a simple program like =hello=?
   #+begin_quote
   Execution requires cooperation between CPU, memory, input/output
   system, graphics/display system, and keyboard for interaction.
   #+end_quote

10. What is a =checksum=?
    #+begin_quote
    A *checksum* is a fixed-size value that is sent/store alongside with
    data, for example precompiled software. At the receiving end, the
    checksum is recomputed and compared with the sent/stored one. If
    the values don't match, the data was likely corrupted or tampered
    with.
    #+end_quote

11. What do the commands =echo $PS1= and =echo $SHELL= mean and return
    (only on Linux)?
    #+begin_example sh
      ## shell prompt definition string variable
      echo $PS1     # on my laptop: \u@marcus@dell:\w $
      ## shell environment variable
      echo $SHELL   # /bin/bash
    #+end_example

** Review: Hardware Organization

1. What is the main role of a *bus* in computer hardware organization?
   #+begin_quote
   A bus acts as a bridge that carries data back and forth between
   system components, typically designed for byte transfers (8-bit
   chunks on a 64-bit system).
   #+end_quote

2. How are *controllers* and *adapters* different in connecting devices to
   the I/O bus?
   #+begin_quote
   A controller is a *chip* located directly *on the device* (e.g., USB
   controller on keyboard), while an adapter is a separate *card* that
   *plugs* into the motherboard (e.g., graphics adapter, WiFi adapter).
   #+end_quote

3. What are the four basic CPU transactions, and what do they accomplish?
   #+begin_quote
   1. Load: copy a byte from main memory into a register.
   2. Store: copy a byte from a register into main memory.
   3. Operate: perform arithmetic/logic using ALU and registers.
   4. Jump: copy a value into the program counter to change execution
      flow.
   #+end_quote

4. What role does CMOS play in a computer system?
   #+begin_quote
   CMOS stores system configuration data (such as BIOS settings, date,
   and time) using a small battery so that the information persists
   even when the computer is powered off.
   #+end_quote

5. Why is main memory (RAM) considered temporary storage, and what
   does it physically consist of?
   #+begin_quote
   RAM is temporary storage because its contents (code and data) are
   lost when power is off. Physically, it consists of DRAM chips.
   #+end_quote

* DONE Week 5: Virtual memory, Kernel probe (Sep 22, 24, 26)
#+attr_html: :width 600px:
[[../img/virtual_memory.png]]

** Review (Friday)

1. What is the main principle of the memory hierarchy?
   #+begin_quote
   Place small, fast storage near the CPU, and larger, slower storage
   far away. The smallest, fastest are called "cache memory".
   #+end_quote

2. Why do SSDs wear out over time? How many writes does a cell have?
   #+begin_quote
   Electron tunneling damages the thin oxide layer in memory cells. A
   cell allows about 100,000 writes (more with error correction).
   #+end_quote

3. Number of NAND memory cells in a 4GB RAM module, approximately?
   #+begin_quote
   Approximately 34 billion densely packed cells: 4 x 2^30 x 8
   #+end_quote

4. What is the OS kernel?
   #+begin_quote
   The OS part that is always in memory, loaded first, controls the
   system resources with interrupt calls (=signal(7)=)
   #+end_quote

5. Which abstractions does the OS provide to applications?
   #+begin_quote
   Processes, virtual memory, file system controlled by the kernel.
   #+end_quote

** Review (Monday)

1. What is the key difference between a thread and a process in terms
   of memory usage?
   #+begin_quote
   Threads share the same code and global data, but each has its own
   stack and registers. Processes have separate memory spaces.
   #+end_quote

2. When are shared libraries (~.so~) linked to the executable?
   #+begin_quote
   Shared libraries (~.so~) are not copied into the executable at
   compile time.  They are dynamically linked at run-time by the
   dynamic linker/loader ~ldd~ when the program starts. This allows
   smaller executables and library updates without recompilation.
   #+end_quote

3. Why is output from multiple C++ threads using ~std::cout~ potentially
   unsafe without a =mutex= ("mutual exclusion", as shown in class)?
   #+begin_quote
   Because the ~ostream~ buffer is not protected, outputs from different
   process threads can become interleaved, confusing the computer.
   #+end_quote

4. What is virtual memory?
   #+begin_quote
   Virtual memory is the abstraction layer that every process
   sees. Inside that address space, the OS arranges different regions
   for program use:
   - =.text= segment – executable code (fixed, read-only).
   - =.data= segment – global and static variables (read/write).
   - Heap – dynamically allocated memory (malloc, new), grows upward.
   - Stack – function call frames and local variables, grows downward.
   #+end_quote

5. What is the big memory illusion of the OS?
   #+begin_quote
   Virtual memory maps each process’s addresses to physical memory, so
   every process sees the same private address space even though
   memory is shared.
   #+end_quote

6. In the C++ memory example, where are the following stored:
   1) =global_var=
   2) =local_var=, =local_var2=
   3) =heap_var=, =heap_var2=
   #+begin_src C++ :main no :includes :results output :exports both
     #include <iostream>
     using namespace std;

     int global_var = 42;

     int main(void)
     {
       int local_var = 1;   // Stack
       int local_var2 = 2;  // Stack
       int* heap_var = new int(99);  // heap
       int* heap_var2 = new int(98); // heap

       cout << "local variables:\n" << &local_var   << endl
            << &local_var2  << endl
            << "heap variables:\n"    << heap_var   << endl
            << heap_var     << endl
            << "global variable:\n" << &global_var  << endl;

       delete heap_var;
       delete heap_var2;
       return 0;
     }
   #+end_src

   #+RESULTS:
   : local variables:
   : 0x7ffee9dc3e50
   : 0x7ffee9dc3e54
   : heap variables:
   : 0x559bfad0aeb0
   : 0x559bfad0aeb0
   : global variable:
   : 0x559bceb14010

   #+begin_quote
   1) ~.data~ segment (global storage)
   2) Stack (local variables disappear when their scope vanishes)
   3) Heap (multiple small =new= calls may come from the same memory)
   #+end_quote

7. If the stack grows downward and the heap grows upward, why don't
   you see this in the previous example?
   #+begin_quote
   OS randomize memory region start addresses for security - this is
   called "Address Space Layout Randomization" (ASLR). Small ~new~
   allocations on the heap may reuse the same memory.
   #+end_quote

   #+begin_src C++ :main no :includes :results output :exports both
     #include <iostream>
     using namespace std;

     int global_var = 42; // in .data segment

     // Recursive function to show stack growth
     void stack_demo(int depth) {
       int local_var = depth;
       cout << "stack depth " << depth
            << " local_var address: " << &local_var << endl;

       if (depth < 5) stack_demo(depth + 1);
     }

     int main() {
       cout << "global variable address: " << &global_var << endl;

       // Observe stack growth
       cout << "\n--- Stack growth ---" << endl;
       stack_demo(1);

       // Observe heap growth
       cout << "\n--- Heap growth ---" << endl;
       int* heap_vars[5];
       for (int i = 0; i < 5; i++) {
         heap_vars[i] = new int(i);
         cout << "heap allocation " << i
         << " address: " << heap_vars[i] << endl;
       }

       // Clean up heap memory
       for (int i = 0; i < 5; i++) {
         delete heap_vars[i];
       }

       return 0;
     }
   #+end_src

   #+RESULTS:
   #+begin_example
   global variable address: 0x562bb9adb010

   --- Stack growth ---
   stack depth 1 local_var address: 0x7ffe6c404a64
   stack depth 2 local_var address: 0x7ffe6c404a34
   stack depth 3 local_var address: 0x7ffe6c404a04
   stack depth 4 local_var address: 0x7ffe6c4049d4
   stack depth 5 local_var address: 0x7ffe6c4049a4

   --- Heap growth ---
   heap allocation 0 address: 0x562bbcdb8ec0
   heap allocation 1 address: 0x562bbcdb8ee0
   heap allocation 2 address: 0x562bbcdb8f00
   heap allocation 3 address: 0x562bbcdb8f20
   heap allocation 4 address: 0x562bbcdb8f40
   #+end_example

   #+begin_quote
   - Stack stride: Each step subtracts 0x30 = 48 (3 * 16^1) from the
     previous address (compiler dependent).
   - Heap stride: Each step adds 32 (2 * 16) (allocator alignment).
   #+end_quote

   #+begin_src C++ :main yes :includes <iostream> <iomanip> :results output :exports both :comments both :tangle yes :noweb yes
     std::cout << 0x7ffd7f8c7384 - 0x7ffd7f8c7354; // stack grows down
     std::cout << std::endl;
     std::cout << 0x5591631c8f40 - 0x5591631c8f20; // heap grows up
   #+end_src

   #+RESULTS:
   : 48
   : 32

8. Where are shared libraries (~.so~) held in memory?
   #+begin_quote
   Shared libraries are mapped into a process’s virtual address space
   in a special region, separate from the program’s own code, data,
   stack, and heap.

   They are loaded into memory once by the dynamic linker and can be
   shared across processes (read-only parts like code), while each
   process gets its own writable copy of data sections.
   #+end_quote

9. Are there also static libraries? What's the difference?
   #+begin_quote
   - A static library (~.a~) is linked at compile-time. Its code is
     copied directly into the executable, so no external file is
     needed at run time. Example: =libm.a= (math functions like =sqrt=).

   - A shared library (~.so~) is linked at run-time, and its code is
     loaded into memory by the dynamic linker. Multiple processes can
     share the same library code in RAM. Example: =libc.so=
     (e.g. =printf=, =pthread=, =malloc=, =free=).
   #+end_quote

10. What is ~systemd~?
    #+begin_quote
    ~systemd~ is the central manager that initializes and controls
    services and resources on a Linux system after the kernel has
    booted (process PID = 1).
    #+end_quote

11. What's the difference between these declarations with regard to
    memory?
    #+begin_example C++
      vec[] {1,2,3};   // array definition (stack)
      vector<int> vec2; // vector definition (heap)
    #+end_example

    #+begin_quote
    - =vec[] {1,2,3};= is a statically allocated object at compile-time.
    - =vector<int> vec2;= is a dynamically allocated object at run-time.
    #+end_quote

12. What do "concurrency" and "parallelism" refer to?
    #+begin_quote
    - *Concurrency* means structuring a program so that multiple tasks
      can make progress during the same period of time usually via
      threads that share code and memory.
    - *Parallelism* means tasks are executed at the same physical time
      on multiple CPU cores or processors with separate code and
      memory.
    #+end_quote

** Review (Wednesday) - Program Memory Layout

1. What memory segments hold program code and global initialized data?
   #+begin_quote
   =.text= (code) and =.data= (global initialized variables).
   #+end_quote

2. How does the heap expand and contract during program execution?
   #+begin_quote
   Via dynamic allocation calls (~malloc/free~ in C, ~new/delete~ in C++).
   #+end_quote

3. What is the role of the dynamic linker (~ld.so~) with shared
   libraries?
   #+begin_quote
   It maps *.so* files into the program’s memory at runtime.
   #+end_quote

4. Where is the user stack located in the virtual address space, and
   how does it behave?
   #+begin_quote
   At the *top* of the address space; it grows and shrinks with function
   calls.
   #+end_quote

5. In C++, where do ~std::vector<int>~ objects and their contents
   reside?
   #+begin_quote
   The object lives on the *stack*, but its dynamic array is stored on
   the *heap*.
   #+end_quote

6. Why do program code and global data always start at the same memory
   address (=0x0=)?
   #+begin_quote
   To ensure predictable linking and loading of executables.
   #+end_quote

7. What happens when you call =new T= in C++?
   #+begin_quote
   It constructs an object of type *T* and allocates memory for it on
   the heap.
   #+end_quote

8. Give an example of two common shared libraries on Linux.
   #+begin_quote
   ~libm.so~ (math functions) and ~libc.so~ (standard C library).
   #+end_quote

9. What tool can show which shared libraries an executable will load
   at runtime?
   #+begin_quote
   The ~ldd~ command.
   #+end_quote
   #+begin_src C :tangle ../src/helloldd.c :main yes :includes <stdio.h> <stdlib.h> <string.h> :results output :exports both :noweb yes
     printf("Hello");
   #+end_src

   #+begin_src bash :results output :exports both
     cd ../src
     make helloldd
     ldd helloldd
   #+end_src

   #+RESULTS:
   : make: 'helloldd' is up to date.
   :    linux-vdso.so.1 (0x00007ffcba3f3000)
   :    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6e427bd000)
   :    /lib64/ld-linux-x86-64.so.2 (0x00007f6e42a07000)

10. How does =vector.push_back= affect memory allocation?
    #+begin_quote
    It may reallocate the internal array on the heap, but stack
    metadata is unchanged.
    #+end_quote

11. Why is the =.text= segment immutable at runtime?
    #+begin_quote
    To prevent accidental or malicious modification of program code,
    ensuring safety and stability.
    #+end_quote

12. Compare how memory is released on the stack vs. the heap.
    #+begin_quote
    Stack memory is released automatically when functions return; heap
    memory must be freed manually (~free~ / ~delete~).
    #+end_quote

13. Why is dynamic linking with shared libraries more efficient than
    static linking?
    #+begin_quote
    It avoids code duplication across programs and reduces executable
    size by loading one shared copy at runtime.
    #+end_quote

14. How does the growth direction of the stack differ from the heap?
    #+begin_quote
    The stack typically grows *downward* (toward lower addresses), while
    the heap grows *upward* (toward higher addresses).
    #+end_quote

15. Why does ~std::vector~ use the heap for its contents instead of the
    stack?
    #+begin_quote
    Because its size is determined at runtime, requiring flexible
    memory that the stack cannot provide.
    #+end_quote

** PRACTICE Memory layout exploration (ca. 30-40 min)

- [X] Open ide.cloud.google.com and go to the terminal.
- [X] Code along with me. Answer the questions.
- [ ] Upload files (=mem.c=, =stack.c= and =Makefile=) as .zip to Canvas.

* DONE Week 6: OS files, Amdahl's Law, Networks (Sep 29, Oct 1, Oct 3)
#+attr_html: :width 400px :float nil:
[[../img/week6.png]]

- [X] *Correction*: ASLR does *not* protect hardware deterioration (it's a
  virtual memory operation). Unnecessary writes (logging, journalling,
  swapping memory) cause these problems. The SSD spreads writes over
  many blocks but not using ASLR.

- [X] *Current assignment*: Remember to upload the ZIP file for the
  memory layout exploration exercise by October 3rd.

- [X] *New assignment*: Reusing OS kernel file descriptors. Submit
  answers to a few questions after experimenting on the shell.

- [ ] Don't forget test 5 (Sunday). Test 6 will go live today.

- [ ] Don't forget the FD demo assignment. Deadline next Wednesday.

** DONE OS files as I/O abstraction

- Files are sequences of bytes.
- Files are used to model I/O devices connected to the computer.
- In this way every application get a uniform I/O device view.

** Whiteboard review: Looking at the per-process file descriptor table

- Check out the code on the whiteboard (5 min)
- Come to the whiteboard
- Explain one line of this code and what it does (if anything)
- Pick any line you like. Use ~//~ for C, and ~#~ for ~bash(1)~ comments.

-----

- Solution:
  #+begin_example C++
    #include <fcntl.h> // file control options for open()
    #include <unistd.h> // system cals like pause(), getpid()
    #include <stdio.h> // I/O like printf(), perror()

    int main() { // main program, no arguments, returns integer

      // open or create demo.txt for writing only, return integer file descriptor
      int fd = open("demo.txt", O_CREAT | O_WRONLY, 0644);
      if (fd < 0) {     // check if file descriptor is zero or positive
        perror("open() call failed.");  // print error message if open() failed
        return 1; // exit with error code
      }
      printf("PID: %d\n", getpid());  // print process ID of this program (to stdout)
      pause(); // keep the process alive
      return 0; // exit without error code
    } // end of main program, delete all program memory including PID
#+end_example

#+begin_example bash
  gcc fd_demo.c -o fd_demo # compile C source file and create object file (executable)
  ./fd_demo  # run executable
  ls -l /proc/<PID>/fd/ # list per-process file descriptor table for PID
#+end_example

** Network adapters as I/O devices



** Amdahl's Law as an upper bound of system improvements

#+attr_html: :width 300px:
[[../img/amdahl.png]]
#+begin_quote
Figure: Amdahl's Law demonstrates the theoretical maximum speedup of
an overall system and the concept of diminishing returns. Plotted here
is logarithmic parallelization vs linear speedup. If exactly 50% of
the work can be parallelized, the best possible speedup is 2 times. If
95% of the work can be parallelized, the best possible speedup is 20
times. According to the law, even with an infinite number of
processors, the speedup is constrained by the unparallelizable
portion.
#+end_quote

* DONE Week 7: Parallelism & Concurrency & Project (Oct 6, Oct 8)

- [X] No Friday classes this week ("Hurkle-Durkle Day")!
- [X] Test 6 is live (deadline Oct 12) - topics: OS files, networking.
- [X] Graded assignments: submit missing, pay attention (or ask me).
- [X] New assignment (deadline Oct 12) - Amdahl's law [review].
- [ ] Topics: Parallelism, Boolean Logic, Hardware Description

** DONE Review: Amdahl's law, network adapters

1. What's Amdahl's law - formula & meaning
   #+begin_quote
   S = 1 / [(1-a) + a/k] where:
   - S = speedup, old over new time spent on a task,
   - a = system part performance (e.g. 60%, a = 0.6),
   - k = part performance improvement (e.g. k = 3).

   Amdahl's law gives an upper bound for the speedup (old over new
   time spent on a task) that can be achieved for a whole system if
   one part of the system is improved.
   #+end_quote

2. What is the maximum speedup that can be achieved according to
   Amdahl's law if we speed a part up to infinity (no time spent)?
   What if we do this for a part that the system spends 1/2 its time
   in?
   #+begin_quote
   For k -> \infty, a = 0.5: S -> 1/(1-0.5) = 2
   #+end_quote

3. Amdahl's law example: You're designing a new CPU that is supposed
   to be 10x as fast as the old one. But you can only improve 50% of
   the CPU, not 100%. How much would this part have to be improved to
   meet the overall performance? How does it follow from Amdahl's Law?
   #+begin_quote
   You have to solve the equation for S for k to obtain
   k=a/(1/S-1+a). For a = 0.5 and S = 10, you get k < 0 - you cannot
   achieve this speedup if you reengineer 50% of the CPU! The best
   possible speedup is S = 2 for any 0 < a < 1.
   #+end_quote

4. Name two programs that can be used to run other programs remotely -
   how are they different from one another?
   #+begin_quote
   ~telnet~ and ~ssh~ - the former is less secure than the latter.
   #+end_quote

5. Which computer architecture abstraction allows us to treat networks
   like input/ouput devices?
   #+begin_quote
   The file abstraction: Network connections are identified by file
   descriptors (called sockets) and accessed using standard protocols
   like FTP (file transfer protocol), or SSH (secure shell).

   That's behind the saying "For the OS, everything looks like a
   file." (Everything has a file descriptor.)
   #+end_quote

** DONE Concurrency and parallelism

1) *Concurrency and parallelism* both aim to improve computing
   performance by running multiple tasks or instructions at the same
   time.

2) Modern processors use *multicore designs* and *hyperthreading* to
   execute multiple threads simultaneously, but parallel performance
   is limited by shared resources and serial program components.

3) *Superscalar processors* can issue multiple instructions per clock
   cycle, but real IPC (instructions per cycle) can be low due to I/O
   delays, pipeline stalls, and short execution times.

4) *SIMD parallelism* allows a single instruction to operate on multiple
   data elements at once, greatly improving performance for
   data-intensive tasks like graphics and audio processing.

** DONE Summary: The Importance Of Being Earnest (when dealing with Abstractions)
#+attr_html: :width 400px :float nil:
[[../img/tiobe.png]]

Plot summary of TIOBE by Oscar Wilde (ChatGPT):
#+begin_quote
Oscar Wilde’s The Importance of Being Earnest is a comedy about two
men, Jack and Algernon, who invent false identities—both named
“Ernest”—to escape social duties and win the women they love. Their
deceptions collide in a web of mistaken identities and witty satire,
ending with Jack discovering his real name is actually Ernest. The
play mocks Victorian hypocrisy and the absurdity of taking appearances
too seriously (read it/watch the 1995? Colin Firth/Rupert Everett film).
#+end_quote

- *Trivial things taken seriously* ("Ernest"): In CS, we obsess about
  naming abstractions (variables, functions, design layers) though
  machines don't care. *Names only matter to humans.*

- *Serious things treated trivially* (identity, marriage): Programmers
  often treat critical abstractions (memory models, thread safety,
  performance assumptions) too lightly, leading to bugs.

- *Masks and identity* (false personas): Abstractions in CS hide
  identity - A file descriptor is "just a number" not the disk head
  moving. A socket is "just a stream", not packets flying across
  networks. The machine reality is messy and requires managing.

- *Critique of hypocrisy* (Victorian hypocrisy): In CS, abstractions can
  be leaky - we pretend that it "just works" (e.g. latency in
  distributed systems), but reality leaks through. Our systems value
  the appearance of simplicity over the complex truth beneath.

Just as Wilde shows that society’s insistence on being “earnest” is
more about appearances than substance, abstractions in CS remind us
that what looks simple and serious on the surface often hides messy,
complex realities underneath.

** DONE What we did achieve so far

| Sessions | Major Topic Area                              |
|----------+-----------------------------------------------|
| 2        | Introduction to Systems & Architecture        |
| 1        | Computer Arithmetic & GDB demo                |
| 2        | Assembly/Disassembly (Hello World + segfault) |
| 1        | Memory Layout & Debugging (struct_t example)  |
| 1        | Low-Level Optimization (copyij vs copyji)     |
| 1        | Computer Networks as I/O                      |
| 2        | Compilation System & Hardware Organization    |
| 2        | OS Abstractions (processes, VM, files)        |
| 1        | Threads & Concurrency                         |
| 2        | Virtual Memory & Memory Segments              |
| 1        | Kernel & Device Modules                       |
| 2        | Networks + DMA, Amdahl’s Law, Concurrency     |
|----------+-----------------------------------------------|
| 18       | *Total*                                         |

** DONE Term projects

- Here are 10 student pair project topics derived directly from the
  past six weeks of CSC 255. Each project involves doing further
  research on the topic and creating a final short demo or
  presentation (10-15 min).

- Your first deliverable is a short informal proposal detailing how
  you will go about researching the topic. We will begin writing the
  proposal today!

- We will check progress in about a month (with a NotebookLM video).

*** Overview

| NO | TOPIC                                   | TEAM | COMMENTS |
|----+-----------------------------------------+------+----------|
|  1 | Assembly across architectures           |      |          |
|  2 | Dissect a segmentation fault            |      |          |
|  3 | Measure memory access performance       |      |          |
|  4 | Visualize virtual memory layout         |      |          |
|  5 | Build and inspect a Linux kernel module |      |          |
|  6 | Explore the ~/proc~ file system           |      |          |
|  7 | Parallelism in practice: Check Amdahl   |      |          |
|  8 | Instruction-level parallelism profiling |      |          |
|  9 | Cache as bridge between CPU and RAM     |      |          |
| 10 | Measure the process-memory gap          |      |          |

*** Guidance

1. Compare assembly outputs across architectures.

   - Focus: Compare the assembly output (=gcc -S=) for =hello.c= or for
     another perhaps even simpler program on two CPU architectures
     (e.g. x86-64 vs. ARM).

   - Deliverable: Annotated comparison showing instruction differences
     and what they reveal about ISA design.

2. Dissecting a segmentation fault.

   - Focus: Use =gdb= to analyze a segmentation fault at the assembly
     level (we did this with the =segfault.c= program).

   - Deliverables: Step-by-step explanation (with screenshots or as
     live demo) of what happens in registers and memory when e.g. a
     ~NULL~ pointer is dereferenced.

3. Measuring memory access performance.

   - Focus: Experiment with different algorithms (we used a simple
     matrix copy) and explain cache performance differences.

   - Deliverable: Timing results, explanation of memory access
     patterns, and visualization of cache-friendly
     vs. cache-unfriendly operations.

4. Visualizing virtual memory layout.

   - Focus: run and extend examples to visualize and understand ~.text~,
     ~.data~, ~heap~ and ~stack~, and shared library segments.

   - Deliverable: Diagram of process memory layout aligned with the
     example, and an explanation of how virtual addresses map to
     physical memory.

5. Building and inspecting a linux kernel module.

   - Focus: Compile and load the =hellokernel.c= module on Linux, or
     another simple module.

   - Deliverable: Short presentation on how user-space programs differ
     from kernel-space modules, plus annotated =dmesg= output.

6. Exploring the ~/proc~ filesystem.

   - Focus: Investigate ~/proc/<PID>/fd~ and ~/proc/<PID>/maps~ for a
     running process.

   - Deliverable: Explain how the file descriptors map to system
     resources, and how this demonstrates the OS "everything is a
     file" abstraction.

7. Parallelism in practice: Amdahl's Law revisited.

   - Focus: Implement an experiment (in C or Python) to measure
     speedup with multiple threads, and compare with Amdahl's
     prediction.

   - Deliverable: Plot of theoretical vs. measured speedup; reflection
     on where Amdahl's law breaks down in practice.

8. Instruction-level parallelism and performance profiling.

   - Focus: Get and explore the [[https://perfwiki.github.io/main/][Linux perf tool]]. Use ~perf stat~ to
     measure instructions per cycle (IPC) for different programs (C or
     Python).

   - Deliverable: Comparative IPC results, identification of
     bottlenecks, and short explanation of superscalar behavior.

9. Cache as bridge: Simulating the memory hierarchy

   - Focus: Implement or simulate a small cache between CPU and RAM;
     measure hit/miss behavior.

   - Deliverable: Table of cache hits vs. misses for sequential
     vs. strided access, and explanation of locality effects.

10. Buses and bandwidth: Measuring the process-memory gap

    - Focus: Use simulation or timing experiments to estimate how bus
      width and clock speed affect data throughput.

    - Deliverable: Calculation or simulation showing the impact of
      8-bit vs. 16-bit bus width on effective memory bandwidth.

*** Pick a team & a topic & start writing the proposal

- Pick a team. (5 min)
- Pick a topic. (5 min)
- Begin writing the proposal. (20 min):

  1) What are your sources going to be?
  2) What's your schedule?
  3) Who can do what between you two?
  4) Questions, concerns, issues?

- Briefly present your thoughts in class. (4 x 5 min)

- Next deliverable: a project update on November 12 in the form of an
  AI-generated video created based on your prompt with
  notebooklm.google.com (free application - time limit).

* DONE Week 8: Boolean Logic (Oct 13, 15, 17)
#+attr_html: :width 600px :float nil:
#+caption: Source: Nishan/Schocken, Nand To Tetris (2021)
[[../img/big_picture2.png]]

- [X] Popquiz review: Digital logic
- [X] Boolean logic video available: [[https://tinyurl.com/boolean-logic-review][tinyurl.com/boolean-logic-review]]
- [X] Review: Digital logic (XOR and Boolean laws)
- [X] Nand2Tetris setup
- [X] Lecture video available: [[https://tinyurl.com/boolean-synthesis][tinyurl.com/boolean-synthesis]].
- [X] Watch this lecture video before the next session.
- [X] Lecture & practice: Boolean function synthesis.
- [X] Test 7 - 30 questions on Boolean logic (Oct 26)
- [X] Home assignment: table->function->code->circuit (Oct 26)
- [ ] Logic gates / interface vs implementation / compositeness

** Review: Digital logic
#+OPTIONS: toc:nil num:nil ^:nil: title:nil author:nil date:nil
1. What are Boolean values and what are the basic Boolean operations?
   #+begin_quote
   - Values: true/false, high/low, on/off.
   - Operations: AND, OR, NOT.
   #+end_quote

2. What is a Boolean expression?
   #+begin_quote
   A Boolean expression is a combination of the basic operations that
   evaluates to a Boolean value, e.g. NOT (0 OR (1 AND 1)) = 0
   #+end_quote

3. Which logical operators correspond directly ot the basic Boolean
   operations in most programming languages?
   #+begin_quote
   ~&&~, ~||~, and ~!~ correspond to AND, OR, and NOT.
   #+end_quote

4. What does this evaluate to: 1 AND (0 OR (NOT(1)))? Write the code
   in any programming language you know, and write the intermediate
   steps, too.
   #+begin_src R :session *R* :results output :exports both
     1 & (0 | (!(1)))
     ## 1 AND (0 OR (NOT(1))) = 1 AND (0 OR 0) = 1 AND 0 = 0
   #+end_src

   #+RESULTS:
   : [1] FALSE

5. Given the formula or function f(x,y,z) = (x AND y) OR (NOT(x) AND
   z), write a program for this function in any language and confirm
   this result:
   #+name: boolean function table
   | x | y | z | f(x,y,z) |
   |---+---+---+----------|
   | 0 | 0 | 0 |        0 |
   | 0 | 0 | 1 |        1 |
   | 0 | 1 | 0 |        0 |
   | 0 | 1 | 1 |        1 |
   | 1 | 0 | 0 |        0 |
   | 1 | 0 | 1 |        0 |
   | 1 | 1 | 0 |        1 |
   | 1 | 1 | 1 |        1 |

   #+begin_src C++ :main yes :includes <iostream> <cstdlib> <string> <iomanip> <vector> :namespaces std :results output :exports both :noweb yes
     bool f(bool x, bool y, bool z);
     int main() {
       vector<bool> v {0,1};
       cout << "| x | y | z | f(x,y,z) |\n"
            << "|---|---|---|----------|\n";
       for (bool a : v) {
         for (bool b : v)
           for (bool c : v)
             cout << "| " << a << " | " << b << " | " << c << " |"
                  << right << setw(9) << f(a,b,c) << " |" << endl;
       }
       return 0;
     }
     bool f(bool x, bool y, bool z) {
       return ((x && y) || (!(x) && z));
     }
   #+end_src

6. Which description of Boolean logic is better - the truth table or
   the close functional form, and why?
   #+begin_quote
   Truth table and functional (formula) are fully equivalent. They
   both describe the entire state space (input-output behavior) of the
   function. Which one you should choose depends on the size of the
   state space: truth tables better for small functions, function
   forms are more compact for larger ones.
   #+end_quote

7. How many states does a Boolean function of 5 arguments have?
   #+begin_quote
   Distributing two values {0,1} over 5 arguments gives 2^5 = 32
   possible states, just like distributing 2 values over 3 states
   x,y,z earlier had 2^3 = 8 states.
   #+end_quote

8. Ture or false? The expression =(x AND y) OR (NOT x AND z)= can be
   represented by two different but equivalent Boolean formulas.
   #+begin_quote
   True - the two equivalent representations are: 1) the Boolean
   algebra form =(x AND y) OR (NOT x AND z)=, and 2) the conditional
   (ternary) or multiplexer form:
   =if x then y else z= -> =x ? y : z= in C/C++.
   #+end_quote

9. Do you remember any Boolean algebra laws? List them alongside their
   definition.
   #+begin_quote
   - Commutative laws for binary operators AND, OR:
     (x AND y) = (y AND x)
     (x OR y)  = (y OR x)
   - Associative laws:
     x AND (y AND z) = (x AND y) AND (x AND z)
     x OR (y OR z) = (x OR y) OR (x OR z)
   - Distributive laws both over AND and OR
     x AND (y OR z) = (x AND y) OR (x AND z)
     x OR (y AND z) = (x OR y) AND (x OR z)
   - De Morgan laws:
     NOT(x AND y) = NOT(x) OR NOT(y)
     NOT(x OR y) = NOT(x) AND NOT(y)
   - Idempotence (doing it twice is the same as doing it once):
     a AND a = a
     a OR a = a
     NOT(NOT(a)) = a    ("double negation")
   - Tautology (always true):
     NOT(a) OR a = TRUE
     NOT(a) AND a = FALSE
   #+end_quote

10. Use the Boolean algebra laws to simplify this expression:
    NOT(NOT(x) AND NOT(x OR y))
    #+begin_example
    NOT(NOT(x) AND NOT(x OR y)) =            De Morgan
    NOT(NOT(x) AND NOT(x) AND NOT(y)) =      Associativity
    NOT((NOT(x) AND NOT(x)) AND NOT(y)) =    Idempotence
    NOT(NOT(x) AND NOT(y)) =                 De Morgan
    NOT(NOT(x)) OR NOT(NOT(y)) =             Double negation/Idempotence
    x OR y
    #+end_example

** Review: Apply Boolean algebra laws (10 min)

- XOR = (NOT(x) AND y) OR (x AND NOT(y)) \equiv  (x OR y) AND NOT(x AND y)

- Use Boolean identities (De Morgan, distributivity, commutativity,
  tautology) to show this.

- I got the idea for this exercise from brilliant.org ([[https://github.com/birkenkrahe/csc255/blob/main/org/3_boolean.org#brilliant-question][see here]]). The
  skill of converting {circuits, tables, functions} is key to being
  able to design CPU elements. Writing code is useful.

- Solution:
  #+begin_example
  f(x,y) = (x \lor y) \land \not (x \land y) \equiv (x \land \not y) \lor (\not x \land y) ?
                     ---------
         = (x \lor y) \land (\not x \lor \not y) <-- De Morgan
                     ----------
         = [x \land (\not x \lor \not y)] \lor [ y \land (\not x \lor \not y)] <-- Distributivity

         = [(x \land \not x) \lor (x \land \not y)] \lor [(y \land \not x) \lor (y \land \not y)] <-- Tautology
            --- 0 ---                          --- 0 ---
         = [ 0 \lor (x \land \not y)] \lor [(y \land \not x) \lor 0 ]

         = (x \land \not y) \lor (y \land \not x) <-- Commutativity

         = (x \land \not y) \lor (\not x \land y)   Q.E.D.
  #+end_example

** Boolean function synthesis exercise

1) Simplify the final expression:
   #+begin_example
   (!x && !y && !z) || (!x && y && !z) || (x && !y && !z)
   #+end_example
2) Build a digital circuit for this function with LogiSim Evolution.
3) Verify the truth table using code (also available in LogiSim).

** Review: Boolean Logic - The Code video - from P to Quantum Computing

*Video:* https://tinyurl.com/boolean-synthesis

1. What's an "easy" computational problem? Example?
   #+begin_quote
   "Easy" (in 'polynomial' time): Instructions/operations can be
   counted - and the result is usually finite (though it may be
   long).

   For example going through a table by rows and columns. For n=3,
   this algorithm contains n x b = 3 x 3 operations. It's time
   complexity is O(n^2) for all n.

   For a table of 1 mio rows and columns, we have 10^12 elements (1
   trillion) - the operation would take no less than 15 minutes
   (1000 secondsassuming each operation takes about 1 nanosecond).

   This is not hard just tedious.
   #+end_quote

   #+begin_src C++ :main yes :includes <iostream> <cstdlib> <string> <fstream> <vector> :namespaces std :results output :exports both :noweb yes
     int main() {
       // Create a 3x3 table
       vector<vector<int>> table = {
         {1, 2, 3},
         {4, 5, 6},
         {7, 8, 9}
       };

       // Range-based loops to print all elements
       for (const auto &row : table) {
         for (const auto &value : row) {
           cout << value << " ";
         }
         cout << endl;
       }

       return 0;
     }
   #+end_src

2. What is a "hard" computational problem? Example?
   #+begin_quote
   If there is no known "polynomial-time" algorithm, you must try all
   possible inputs to find one that satisfies a given condition.

   Here is an example of "Boolean satisfiability": Is there an
   assignment of TRUE/FALSE values to x,y,z that make the whole
   formula true?

   (x \lor y) \land (y \lor z) \land (x \lor z) = f(x,y,z)

   You know that there are 2^3 = 8 combinations - that's exponential,
   not polynomial - very large for very large n. For n = 1 mio, even
   at an unreal speed of 10^12 (1 trillion) assignments per second,
   this would take 10^310000 years (age of the universe: 10^10).
   #+end_quote

3. So in a nutshell, what's the difference between easy and hard
   computational problems?
   #+begin_quote
   Easy problems can be solved by devising efficient algorithms - like
   sorting a list - and running them on fast machines. Solvable
   quickly (P).

   Hard problems require checking every possible combination of inputs
   (like a complete truth table) - time grows exponentially. Only
   checkable quickly, or NP-complete.
   #+end_quote

4. Is the P vs. NP issue of practical importance?
   #+begin_quote
   Very practical: easy problems scale to large inputs - we can sort
   billions of numbers or search vast datasets super-fast. Hard
   problems explode - even 100 variables cannot be brute-forced.

   If a task is NP-hard, we design heuristics, approximations, or
   probabilistic methods. Approaching the truth must be enough.

   It determines what problems are tractable vs. requiring compromises
   in the real world. All of automation (and of AI) depends on that.
   #+end_quote

5. Show that =x OR y= can be expressed in terms of =AND= and =NOT= using the
   truth table and the Boolean identities - steps to do this?
   #+begin_example
   1) build a funnction from the truth table

     | x | y | f(x,y) | f(x,y)       |
     |---+---+--------+--------------|
     | 0 | 0 |      0 |              |
     | 0 | 1 |      1 | NOT(x) AND y |
     | 1 | 0 |      1 | x AND NOT(y) |
     | 1 | 1 |      1 | x AND y      |

     f(x,y) = (NOT(x) AND y) OR (x AND NOT(y)) OR (x AND y)
            = (\not x \land y) \lor (x \land \not y) \lor (x \land y)

   2) Simplify the function:

     f(x,y) = (\not x \land y) \lor x \land (\not y \lor y)
            = (\not x \lor x) \land (y \lor x)
            = y \lor x = x \lor y

   3) Nothing gained! Need to remember the De Morgan identity:

      f(x,y) = x \lor y = \not ( \not x \land \not y)

   Btw. for more than 2-3 variables, Karnaugh maps are a more efficient
   way to derive logical functions (you did that in CSC 245/Digital Logic).
   #+end_example

6. What's the "atom" at the heart of physical computer architecture?
   #+begin_quote
   It's the Nand gate - the OR operation is redundant (actually,
   either the OR or the AND operation is redundant - Nor works, too).

   Another way of putting this: Nand is functionally complete.
   #+end_quote
   #+attr_html: :width 400px :float nil:
   [[../img/nand_vs_and_not.png]]

7. How can you prove that Nand is functionally complete?
   #+begin_quote
   You have to prove that every Boolean function can be expressed in
   terms of NAND - decomposes into proving this for NOT and AND:

   1) \not x = \not (x \land x) = x Nand x

   2) x \land y = \not (\not (x \land y)) = \not (x Nand y)
   #+end_quote
   #+attr_html: :width 400px :float nil:
   [[../img/nandx.png]]

   #+attr_html: :width 400px :float nil:
   [[../img/xnandy.png]]

8. Is this notion of completeness relevant elsewhere in computing?
   #+begin_quote
   A "Turing-complete" programming language is an analogous concept:
   such a language can express any computation that can be done by a
   Turing machine (tape + head + state + program).

   Neural networks with non-linear activation functions are universal
   approximators: They can approximate any continuous function. This
   does not tell you how many neurons are needed to do this.
   #+end_quote

9. Doesn't quantum computing change all of this?
   #+begin_quote
   "Quantum computing" (computing with more than two logical values)
   chenges how we compute but not what is computable.

   Quantum computers are still not more than Turing-complete: It can
   be shown that every quantum computation can be simulated by a
   classic Turing machine (at much slower speed).
   #+end_quote

10. Does quantum computing change the P vs. NP (easy vs. hard) issue?
    #+begin_quote
    Some specific problems that seem hard classically (NP) can be
    reclassified from "practically hard" (checking in exponential
    time) to efficiently solvable. Example: Shor's algorithm (used for
    encryption).

    Summary table:
    | Problem class            | Classical (P) | Quantum (BQP) | NP status   |
    |--------------------------+---------------+---------------+-------------|
    | Sorting, arithmetic      | Easy          | Easy          | In P        |
    | Factoring                | Hard          | Easy (Shor)   | In NP       |
    | Search                   | Hard (O(N))   | √N (Grover)   | In NP       |
    | SAT, TSP, graph coloring | Hard          | Still hard    | NP-complete |
    #+end_quote

** Boolean normal forms, computability, NAND gates
#+attr_html: :width 400px :float nil:
[[../img/video2.png]]

* DONE Week 9: Hardware Description Language (Oct 20, 24)
#+attr_html: :width 400px :float nil:
#+caption: Nand2Tetris HDL simulator dashboard
[[../img/hdl.png]]

- [X] Review: Boolean logic review
- [X] Logic gates
- [X] Interface vs. Implementation
- [ ] Term project check-in (NotebookLM video) by Nov 14
- [ ] Hardware Description Language
- [ ] Nand2Tetris HDL Simulator
- [ ] Next week: Building our first chips

** Review - Boolean logic

Everybody! Name one topic, fact, formula or code chunk from last week!

Example topics:
#+begin_quote
1. NAND gate/function/diagram:

   | a | b | Nand(a,b) | And(a,b) |
   |---+---+-----------+----------|
   | 0 | 0 |         1 |        0 |
   | 1 | 0 |         1 |        0 |
   | 0 | 1 |         1 |        0 |
   | 1 | 1 |         0 |        1 |

2. Boolean Satisfiability (SAT) - first NP-complete problem (1971):
   Can we make a given logical formula true?

3. P vs NP - Polynomial solvability vs. Non-polynomial checkability.

4. Quantum Computing with non-Boolean, quantum-theoretic truth
   values.

5. RAG (Retrieval Augmented Generation) - context provider for
   Generative AI.
#+end_quote

Example facts:
#+begin_quote
1. Every Boolean function can be represented using an expression
   containing only NAND operations (provable).

2. Every Boolean expression reduces to one of two Boolean values (axiomatic).

3. Truth tables are useful when the state space is small (2-3
   variables).
#+end_quote

Example formulas:
#+begin_quote
1. De Morgan: (NOT(x) AND y) = (x and NOT(y))
2. Tautology: NOT(a) OR a = TRUE
3. Idempotence: NOT(NOT(x)) = x
4. Self-NAND property: NAND(x,x) = NOT(x AND x) = NOT(x)
#+end_quote

* DONE Week 10: Project 1: Basic circuits

- [X] Term project update using NotebookLM.

- [X] Review: HDL - Do It Yourself at [[https://tinyurl.com/review-hdl][tinyurl.com/review-hdl]] - Make
  copy, fill in the answers, upload your URL to Canvas by 3:20 pm.

- [X] Building the Nand gate from start to finish.

- [X] *Lab:* Building the And, Or, Not, and Xor chips from Nand.

** Term project update
#+attr_html: :width 400px :float nil:
[[../img/projectUpdate.png]]

- Give me a short overview of your project progress.

- You've only got 4 weeks (not counting Thanksgiving week).

- You should try to finish this before the last week.

- I created a mock-up project proposal for one of the projects.

- The assignment lists some notes for each of your projects.

- The assignment also includes your deliverables (by Nov 14):

  1. Project proposal (you should develop this - details, references)
  2. Project report log (accomplished/challenges/next steps)
  3. Prompt that links the proposal to the project log

- You have some time on November 12 (class cancelled) to finish this.

** Review questions: HDL

1. What does HDL stand for, and what is its primary purpose?
   #+begin_quote
   HDL stands for *Hardware Description Language* and is used to
   describe the structure and behavior of hardware circuits.
   #+end_quote

2. In HDL, what is the =CHIP= data structure like when compared to C or
   C++?
   #+begin_quote
   The =CHIP= data structure in HDL is similar to a ~struct~ in C or a
   ~class~ with ~public~ members in C++ - it has data members and methods.
   #+end_quote

3. In HDL, what does the keyword =PARTS:= introduce?
   #+begin_quote
   The list of *components (gates or subchips)* used to build the
   current chip - the implementation details.
   #+end_quotea

   4. In HDL terms, what part of a chip design is comparable to a
      function's declaration in programming?
      #+begin_quote
      The stub file - it declares the chip's interface (API) but not its
      implementation.
   #+end_quote

5. What type of programming paradigm best describes HDL?
   #+begin_quote
   HDL is *functional* or *declarative* — it describes what exists, not
   what executes. It is not *procedural* and not *object-oriented*.
   #+end_quote

6. Why is the order of statements in HDL significant?
   #+begin_quote
   Because HDL describes a *static circuit diagram* where wiring order
   determines connectivity and meaning.
   #+end_quote

7. What are the three main ways to run the HDL hardware simulator?
   #+begin_quote
   *Interactive*, *script-based*, and *with or without compare/output
   files*.
   #+end_quote

8. What files are typically provided for each chip in a Nand2Tetris
   project?
   #+begin_quote
   A =.hdl= stub file, a =.tst= test script, and a =.cmp= compare file. Here
   is the stub file for the Xor gate:
   #+end_quote
   #+begin_example
   // This file is part of www.nand2tetris.org
   // and the book "The Elements of Computing Systems"
   // by Nisan and Schocken, MIT Press.
   // File name: projects/1/Xor.hdl
   /**
   * Exclusive-or gate:
   * if ((a and Not(b)) or (Not(a) and b)) out = 1, else out = 0
   */
   CHIP Xor {
       IN a, b;
       OUT out;

       PARTS:
       //// Replace this comment with your code.
   }
   #+end_example

9. How are compare (=.cmp=) files generated?
   #+begin_quote
   By evaluating the equivalent Boolean functions for all input values
   (aka "behavioral simulation") using a correct implementation. You
   can do this with C++ or Python code, to produce the expected truth
   table, or by simulating the circuits in LogiSim with the analyzer.
   #+end_quote

10. What is the purpose of the symbolic link created with a command
    like =ln -s /nand2tetris/tools/HardwareSimulator.sh hdl= in a
    directory that is in the ~$PATH~?
    #+begin_quote
    To make the HardwareSimulator shell script executable from any
    directory in the ~$PATH~ using the command =hdl=.
    #+end_quote

** Practice: Building the NAND gate

See videos including software download/install in Canvas.

1. [X] Model the gate in LogiSim Evolution.
2. [ ] Extract the compare file from LogiSim Evolution.
3. [X] Write HDL file in an editor.
4. [ ] Open the HDL simulator software.
5. [ ] Load HDL file into the simulator.
6. [X] Run HDL file in the simulator.
7. [ ] Write test script in an editor.
8. [ ] Load test script into the simulator.
9. [ ] Load the compare file into the simulator.
10. [X] Run test script against the compare file.

If the last step passed successfully, zip .hdl, .cmp, and .tst files
and submit the .zip file to Canvas.

** Rationale for project 1

A typical computer architecture is based on a set of elementary logic
gates like And, Or, Mux, etc. as well as on their bitwise versions
And6, Or16, Mux16, etc. (assuming a 16-bit machine).

In this project, you will build a typical set of basic logic
gates. These gates form the elementary building blocks from which you
will build the Hack computer's CPU and RAM chips in later projects.

*** Constraints

- Prerequisites:
  1. [ ] Learn the basics of Hardware Description Language (HDL).
  2. [ ] Learn the basics of the HDL simulator (online/offline).

- *Outcome:* Text files with the HDL code of 15 chips to be built.

- *Caveat:* The names of the HDL files to be submitted must be the exact
  file names that appear in the nand2tetris/projects/01 folder on your
  computer.

- Simply edit the =*.hdl= files in that folder using any text editor,
  write the HDL code in them, and save them using their given manes,
  or enter the code straight into the Web IDE.

- You can check the scripts yourself using the provided tests in =*.tst=
  files (so you don't have to write these yourself).

- The Hardware Simulator is described in a short online =Guide=.

*** Objectives

Build the following chips (aka gates):
1) Nand (given - don't have to build)
2) Not (from Nand only)
3) And (from Nand only)
4) Or (from Nand only)
5) Xor (from Nand only)
6) Mux (from Nand using previously created chips)
7) DMux
8) Not16
9) And16
10) Or16
11) Mux16
12) Or8Way
13) Mux4Way16
14) Mux8Way16
15) DMux4Way
16) DMux8Way

Since Nand is considered primitive, there is no need to implement it
(we'll do it anyway as a demo).

*** Files

- For each chip Xxx in the list, a skeletal =Xxx.hdl= program, also
  called a *stub* file, is provided. Its PARTS (implementation) section
  is missing.

- For each chip Xxx in the list, a =Xxx.txt= script is provided that
  tells the hardware simulator how to test the chip, along with a
  =Xxx.cmp= compare file containing the correct outputs that the
  supplied test is expected to generate. Your task is writing, and
  testing, the chip implementations.

*** Contract

- For each chip Xxx in the list, the chip implementation (modified
  =Xxx.hdl= file), tested by the supplied =Xxx.tst= file, must generate
  the outputs listed in the supplied =Xxx.cmp= file (a truth table).

- If the outputs generated by your chip disagree with the desired
  outputs, the simulator will report error messages.

*** Online IDE

- All the =Xxx.hdl=, =Xxx.tst=, and =Xxx.cmp= files are available in browser
  memory.

- You cannot upload and test any other than the pre-set chips.

- If you mess up the files in the Online IDE, you can go to the
  =Settings= Tab and choose =Reset=.

*** Local App

- This software is *not* needed for the Nand2Tetris projects but it
  teaches you the underlying principles of the Hardware Description
  Language and the simulator so we'll do it in class together.

- To develop and test your own chips, you need to download and install
  the local HDL simulator software (for Mac, Windows or Linux) from
  here: [[https://tinyurl.com/nand2tetris-simulator][tinyurl.com/nand2tetris-simulator]] (see below for details).

- Download the software including the =project= and =tools= directories as
  a =zip= file. Unpack it. The simulator is started with the =.bat=
  (Windows) or the =.sh= file (Mac/Linux).

- I created a symbolic link from the script to an executable:
  =~/nand2tetris/tools/HardwareSimulator.sh= to =~/.local/bin/hdlsim=.
  And now I can run =hdlsim= from anywhere on my machine.

** Building And, Or, Not, Xor chips from Nand in the Web IDE

For the remainder of the week:

- Begin by building the Not chip using only Nand
- Next, build And
- Next, build Or
- Next, build Xor

Building these will be easier and faster because you can use the Web
IDE, which has =Nand= built-in as your building block.

Feel free to do this in the desktop software instead (that's what I
do, because it gives me more control and furthers my insight).

These builds are graded: Submit your files in one Zip file
=project1.zip= in Canvas. Each submission should contain:
#+begin_quote
1) Xxx.circ - Generate first in LogiSim Evolution and label pins and wiring
2) Xxx.png - Screenshot of the IDE with the completed =.hdl= file and the msg:
#+end_quote
[[../img/success.png]]

** Hints: Building NOT with NAND

1) Start by creating the logic circuit in LogiSim.
2) Remember (a NAND a) = NOT(a)
3) Label all pins and wires.
4) You only have to create the implementation ~PARTS~.
5) Check out the built-in chip (answer hidden).
6) Your answer (one line only) should use this template:
   #+begin_example
   Nand(a= , b= , out= )
   #+end_example
7) For AND, OR, XOR, remember the NAND identities from class:
   - a AND b is realized with three NAND gates.
   - a AND b is realized with three NAND gates, too.
8) This took me about 1/2 hour each for a full solution.

* DONE Week 11: Basic Circuits continued (Project 1)
#+attr_html: :width 600px :float nil:
#+caption: Xor gate as circuit diagram and HDL simulator files (Source: Nisan/Schocken 2020)
[[../img/xor_all.png]]

- [X] Remember to submit (*Nov 5*): Building NAND in the HDL Simulator.
- [X] Resubmit 100% correct solutions for NOT, AND, OR, XOR as Nand.
- [X] Home assignment: Finish Project 1 (by *Nov 17*).

- [X] *Lecture:* Intro to multi-bit buses, esp. Not16 and And16.
- [X] *Lab:* Building the PARTS of the And16 CHIPs as Nand CHIPs.

- [X] *Lecture:* Intro to multiplexers and demultiplexers.
- [X] *Lab:* Building the PARTS of the Mux CHIP

** Review: And as a function of Nand

1. [X] Do the algebra.
2. [X] Draw the diagram.
3. [X] Label the diagram.
4. [X] Design the chip PARTS.
5. [X] Run the tests.

** Build the Xor chip from Nand chips

- Try it on your own first.

- There are three other methods to do this:
  #+begin_quote
  1) Load balancing (leads to a 5-Nand solution)
  2) Algebraic derivation (leads to a 4-Nand solution)
  3) Using previously built chips (5-Nand solution)
  #+end_quote

** Home assignment: Finish all basic circuits (multi-bit and multi-plex)

- To do: Mux, DMux, Not16, And16, Or16, Mux16, Or8Way, Mux4Way16,
  Mux8Way16, DMux4Way, DMux8Way.

- I will introduce the circuits to you with a short lecture (Wed).

- You only need to submit a single (clearly visible) screenshot of
  your working, tested Hardware Simulator dashboard with the CHIP.

- You have until after the deadline for the term project review
  (Nov 14) to finish this - submission deadline is Nov 17.

* DONE Week 12: Boolean Arithmetic and term project review - Nov 10, 14

- [X] No meeting this Wednesday: Work on your projects!

- [X] Test 8: Multi-bit chips, multiplexers and de-multiplexers
- [X] Boolean arithmetic: numbers, operations, addition
- [X] Adders and incrementer for the ALU

- [ ] Negative numbers and Two's Complement method
- [ ] Arithmetic-Logic-Unit for the CPU
- [ ] Home project assignment: Build Adder chips (+Nov 21+)

** Term project review
#+attr_html: :width 400px :float nil:
[[../img/review.png]]

*Project-specific notes:*

- Dissect a segmentation fault: Show how you identified the cause,
  tools used (e.g., gdb, valgrind), and what insights you gained about
  memory or pointer errors.

- Visualize virtual memory layout: Demonstrate your visualization
  output or explain what patterns you discovered in memory regions
  (stack, heap, text, data).

- Build and inspect a Linux kernel module: Summarize what you learned
  about the kernel build process, module loading/unloading, and
  debugging strategies.

- Parallelism in practice: Check Amdahl’s Law: Present your
  experimental setup, timing results, and how they compare to
  theoretical speedup predictions.

- Measure the process-memory gap: Describe your measurement method
  (e.g., using /proc, ps, or timing tools) and discuss what the
  results reveal about virtual vs. physical memory.

* DONE Week 13: Binary addition - Nov 17, 19, 21
#+attr_html: :width 400px :float nil:
[[../img/alu.jpg]]

- [X] Binary operations
- [X] Binary addition
- [X] Building adder hardware
- [ ] +Negative numbers and Two's Complement+
- [ ] Project 2: Adder designs (deadline now *December 7*)
- [ ] Test 9: Binary arithmetic and adder design
- [ ] +Next: ALU specification+
- [ ] Bonus: Distributed event streaming with Apache Kafka

*Don't forget your term projects: Sprint review assignment coming soon!*

** Review: Binary arithmetic I

1. How does the computer deal with very, very large numbers?
   #+begin_quote
   By splitting the number up across multiple registers each of which
   adheres to maximum floating-point precision for representation of
   numbers.
   #+end_quote

2. What is "overflow" for a computer?
   #+begin_quote
   The result of an artihmetic operation like 65,535 + 1 on a 16-bit
   computer exceeds the range of values that can be represented by the
   fixed number of bits allocated for that number in memory.
   #+end_quote

3. How many rows has a table for numbers represented with 4 bits?
   #+begin_quote
   2^4 = 16 rows for each of the 4 bits taking one of the values 0,1
   #+end_quote

4. Convert 73 to 8-bit binary.
   #+begin_quote
   - Solution: 64 (2⁶) + 8 (2³) + 1 (2⁰) = 0100 1001.
   #+end_quote

5. Convert binary 0110 0100 to decimal.
   #+begin_quote
   - Solution: \(1×2^6 + 1×2^5 + 0×2^4 + 0×2^3 + 1×2^2 + 0×2^1 + 0×2^0
     = 64 + 32 + 4 = 100\).
   #+end_quote

** Review Binary arithmetic II

1. What's to be preferred: Build special hardware or design special
   software?
   #+begin_quote
   Usually, software over hardware design whenever possible though
   there are exceptions (e.g. large matrix multiplications for deep
   learning tasks, or graphics generation).
   #+end_quote

2. How many bits does the half-adder add?
   #+begin_quote
   #+attr_html: :width 400px :float nil:
   [[../img/half_adder.png]]
   #+end_quote

3. How many bits does the full-adder add?
   #+begin_quote
   #+attr_html: :width 400px :float nil:
   [[../img/full_adder.png]]
   #+end_quote

4. Why do we need a full adder?
   #+begin_quote
   Because binary addition of two multi-bit numbers always produces a
   carry, and at every bit position after the least significant bit
   you must add *three* values: the bit from a, the bit from b, and the
   carry-in from the previous (less significant) position.  The
   half-adder cannot do the 3-input addition. It only handles the LSB
   position.
   #+end_quote
   Example:
   #+begin_example
   a = 1              half-adder: a + b = 1 + 1 = 0
   b = 1              full-adder: a + b + c = 1 + 1 + 1 = 10
   c = 1 (carry-in)
   #+end_example

5. Which chip can add two 3-bit numbers?
   #+begin_quote
   #+attr_html: :width 400px :float nil:
   [[../img/16_bit_adder2.png]]

   A custom chip =Add3= which inputs =a[3]= and =b[3]= and outputs =out[3]=.
   #+end_quote
   Example:
   #+begin_example
   A = 101 (5)            1 0 1
   B = 011 (3)        +   0 1 1
                      ---------
   a[0]+b[0]   LSB            0   1+1=0   out[0] (half-adder) carry = 1
   a[1]+b[1]   Bit 1        0     0+1+1=0 out[1] (full-adder) carry = 1
   a[2]+b[2]   MSB        1       1+0+1=0 out[2] (full-adder) carry = 1
                      = 1 0 0 0
   #+end_example

* Week 14: Distributed Processing Architecture - Dec 1, 3, 5
#+attr_html: :width 400px :float nil:
[[../img/kafka.jpg]]

- [X] Some of you are missing assignments: Submit late for 50%
- [X] Final presentation: *Schedule date*, and *share link* for materials
- [X] Deadline for materials (if you want a grade): December 12, 2025
- [X] Test 9 is available (binary arithmetic/adders in HDL)
- [X] Change of topic: Distributed Computer Architecture

- [ ] *Please fill in the course evaluation now! (5 min)*  
- [ ] Test 10: Apache Kafka (next week)

** DONE Why change course?

The idea came to me first after I (recently) dug more deeply into data
engineering where the relationship between hardware and software
matters most and is most at the surface.

Apache Kafka is a software system that is closest to the metal of the
hardware. It will serve as an introduction to distributed computer
architectures. The specific use case is big data streaming and
building resilient data pipelines.

Apache Kafka (and other platforms like it) runs as software on servers
built around the von Neumann computer architecture (CPU + RAM + I/O),
and leverages the hardware elements for fast streaming of big data.

** DONE Part 1: Distributed computing architectures

- Challenges
- Lambda architecture
- Kappa architecture
  
** IN PROGRESS Part 2: Intro to Apache Kafka & Installation

- Zero to Hero: Apache Kafka (video analysis)
- Apache Kafka concepts (viz von Neumann architecture)
- Installing Apache Kafka on Google Cloud Shell [Linux or Windows]

** Part 3: Using Apache Kafka

- Batch vs streaming & logging
- Keys & message serialization
- Scale & consumer groups


** NEXT Review Data Architecture and Theory

[I asked Gemini to produce 5 review questions based on the whiteboard
screenshots from the last class.]

1. *The "Big Data" Definition:* On the board, we broke down "Big Data"
   into the "5 Vs." We categorized them into two groups: "Measurable"
   and "Qualitative." Which 'Vs' fell into the *Measurable* category?
   #+begin_quote
   *Answer:* Volume (size), Variety (format), and Velocity (streaming).
   (The Qualitative ones were Veracity and Value).
   #+end_quote

2. *Pipeline Stages:* According to the "Data Pipeline" diagram we drew,
   what is the correct chronological order of the four main stages of
   data movement?
   #+begin_quote
   *Answer:* Inject (or Ingest) -> Store -> Transform -> Analyze.
   #+end_quote

3. *Lambda Architecture:* In the Lambda Architecture diagram, incoming
   data is split into two distinct processing paths (layers) before
   reaching the "Serving Layer." What are these two layers called?
   #+begin_quote
   *Answer:* The *Batch Layer* (associated with human/historical analysis)
    and the *Speed Layer* (associated with machine/real-time analysis).
   #+end_quote

4. *Kappa Architecture:* How does the Kappa Architecture differ
   fundamentally from the Lambda Architecture in terms of its
   processing layers?
   #+begin_quote
   *Answer:* Unlike Lambda, which has two paths (Batch + Speed), Kappa
    simplifies this into a single *"Real-time" layer* (Streaming) using
    tools like Apache Kafka for both processing and storage.
   #+end_quote

5. *Scaling Strategies:* On the board, we drew a diagram of "Houses"
   with vertical and horizontal arrows. What is the difference between
   the "Vertical" approach (making the house bigger) and the
   "Horizontal" approach (building more houses), and which one is
   Kafka designed for?
   #+begin_quote
   *Answer:*
   - *Vertical Scaling (Scale Up):* Buying a bigger, more expensive
     computer (adding RAM/CPU to one node).
   - *Horizontal Scaling (Scale Out):* Adding more cheap computers
     (nodes) to the cluster.
   - *Kafka's Choice:* Kafka is a *Distributed System*, so it relies on
     *Horizontal Scaling*.
   #+end_quote

** TODO Review Apache Kafka Basics

1. I tested the scripts in a folder =kafka= that contains all the
   installation, the =data= storage and the metadata. If I now rename
   =kafka= -> =kafka2=, do I have to make any changes?

   #+begin_quote
   All links inside =kafka= are relative links. The only change you have
   to make (not while running a Kafka instance - you must stop the
   server) is to change =log.dirs= in =config/kraft/server.properties= to
   =/home/birkenkrahekafka2/data= (better to use an absolute path here).
   #+end_quote
   

* Week 15: Your project presentations
