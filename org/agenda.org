#+TITLE:Agenda - Computer Architecture
#+AUTHOR:Marcus Birkenkrahe
#+SUBTITLE:CSC 255 Agenda Fall 2025
#+STARTUP: overview hideblocks indent
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
#+PROPERTY: header-args:python :session *Python* :results output :exports both :noweb yes
#+PROPERTY: header-args:C :main yes :includes <stdio.h> :results output :exports both :noweb yes
#+PROPERTY: header-args:C++ :main yes :includes <iostream> :results output :exports both :noweb yes
* DONE Week 0: Overview

- Tools used and plans made
- Motivation to enrol in the course

* DONE Week 1: Overview: Why computer architecture, LogiSim installation

- [X] Reviews
- [X] Installation: LogiSim
- [X] Nand2Tetris projects
- [ ] *Research mixer on Tuesday, September 30 @Lyon - tinyurl.com/res-mix*
- [ ] Why Computer architecture?

** DONE Review Week 0

1. Which tools are we going to use in this course?
   #+begin_quote
   - Hardware Description Language (HDL)
   - LogiSim (digital circuit simulator)
   - GDB (GNU DeBugger) and GCC (GNU C Compiler)
   #+end_quote
2. What stands between source code and machine code?
   #+begin_quote
   Source code (in any high level language) is compiled into
   human-readable Assembly code and then linked to not human-readable
   object code for execution. (Though some compilers generate object
   code directly - Assembly is not always exposed.)
   #+end_quote
3. What is specialy about this intermediate step?
   #+begin_quote
   What's special about Assembly is that it depends on the CPU. Each
   machine has an instruction set architecture (ISA) that is
   specific. The assembly file and the object code of one CPU will not
   run on another.
   #+end_quote
4. Which programming languages are portable?
   #+begin_quote
   No language is "portable". Their implementation makes them
   portable:

   - C and C++: The source code is portable, but the compiled binary
     is not. You need a compiler for each target architecture.
   - Python and JavaScript: Interpreted (or Just-In-Time compiled) -
     source runs cross-platform if the interpreter is available.
   - Java: uses intermediate bytecode that runs on a Java Virtual
     Machine (JVM), which is ported to many hardware platforms.
   #+end_quote
5. What is the Gestalt principle, and how does it apply to computer
   architecture?
   #+begin_quote
   The Gestalt (German for 'shape') principle comes from psychology:
   "The whole is greater than the sum of its parts." It means that we
   perceive systems as unified structures, not just isolated
   components.

   In computer architecture, the Gestalt idea applies because a
   computer is not just CPU + memory + I/O in isolation. The
   organization and interaction of these components give rise to
   capabilities (like parallelism, pipelining, or memory hierarchies)
   that cannot be understood by looking at the parts alone.
   #+end_quote

** DONE Syllabus

- New attendance policy and app (QR code)
- Updated AI guidelines
- Project-centered grading
- Textbooks (optional): CS:APP / Nand2Tetris

** DONE Schedule

| Week(s) | Topic                           | Notes                             |
|---------+---------------------------------+-----------------------------------|
| 1       | Intro to Systems & Architecture | Abstractions, tools, setup        |
| 2–3     | Project 1: Boolean Logic        | HDL, logic gates, test scripts    |
| 4–5     | Project 2: Combinational Chips  | Adders, ALU parts, multiplexers   |
| 6–7     | Project 3: Sequential Chips     | Flip-flops, RAM, memory hierarchy |
| 8–9     | Project 4: Machine Language     | Hack code, write simple programs  |
| 10–11   | Project 5: Hack CPU             | Build CPU, connect components     |
| 12–13   | Project 6: Assembler            | Write assembler, resolve symbols  |
| 14      | Integration & Demo Prep         | I/O mapping, final integration    |
| 15      | Hack Computer Demo              | Demo: HDL → CPU → Assembler stack |

** DONE Projects

*** The Idea of Nand2Tetris
#+attr_html: :width 400px :float nil:
[[../img/michelangelo.png]]

*** The course projects (Hack hardware)
#+attr_html: :width 600px :float nil:
#+Caption: Nand2Tetris projects we may be able to complete in CSC 255.
[[../img/projects.png]]

*** Other projects (Hack software)
#+attr_html: :width 600px :float nil:
#+Caption: Nand2Tetris projects that we will not cover in CSC 255.
[[../img/projectsOS.png]]

** DONE Install LogiSim

- [[https://cdn.hackaday.io/files/1814287762215552/logisim_tutorial.pdf][The Guide to Being a LogiSim User (tutorial)]]

- Go to [[https://github.com/logisim-evolution/][github.com/logisim-evolution/]]

- Navigate to the [[https://github.com/logisim-evolution/logisim-evolution/releases][releases repo]]

- Download what you need for Windows (msi), MacOS (dmg), or Linux
  (rpm)

- You must have Java installed (in a terminal, run =java --version=)

- On Windows, double-click the .msi file and change the location to
  your local disk (=C:/Users/yourname=). Or download and run the .jar
  file.

- On MacOS, double-click the .dmg file then drag the Logisim-Evolution
  icon into the Applications folder.

- On Linux, if you're on Debian, download the .deb file and run
  #+begin_example
  sudo dpkg -i logisim-evolution-3.9.0_[cpu].deb
  #+end_example

- Check that your program exists and open it to create your first
  diagram.


** DONE Research mixer - why you should do this
#+attr_html: :width 400px :float nil:
[[../img/research_mixer.png]]

- Doing research is a great excuse to build a relationship with a
  business: People love to help, especially techies!

- This is practical networking that can help you get an internship or
  a job: You can put it on your resume, too!

- Through research, you learn to update your knowledge, and you may
  find out about interesting applications that define your career!

- Science is at crossroads for multiple reasons: It's too bloated,
  it's too expensive, it's getting too difficult, and there's AI.

** DONE Brilliant course
#+attr_html: :width 400px :float nil:
[[../img/brilliant2.png]]

- Start the free [[https://brilliant.org/courses/digital-circuits/]["digital circuits" course on brilliant.org]] to
  revitalize your digital logics knowledge and to get a view of all
  the different aspect of the CPU.

- I am on day 56 currently, starting with Nand gates - which we're
  assuming as prerequisite knowledge here.

- Includes 3 daily practice questions. You can finish in 1 month.

** DONE Review (Monday)

1. What is a compiler flag - can you give an example?
   #+begin_src bash :results output :exports both
     cd ../src
     echo -e "#include <iostream>\nint main(){\
     std::cout<<\"Hello, world!\";return 0;}" > hello.cpp
     cat hello.cpp
     g++ hello.cpp -o hello
     ./hello
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main(){std::cout<<"Hello, world!";return 0;}
   : Hello, world!

2. What does a digital NOT circuit consist of?
   #+begin_quote
   1) An input pin (Voltage ON/OFF)
   2) An output pin (Voltage OFF/ON)
   3) A Not gate

   [[../img/Not.png]]
   #+end_quote

3. What kind of application is LogiSim Evolution?
   #+begin_quote
   - Free open source software (FOSS)
   - Java application (.jar)
   - Digital Circuit simulation software
   #+end_quote

4. Discuss this with your neighbor and come to a presentable
   conclusion: Do logical statements, truth tables, and circuit
   diagrams express *exactly* the same information? For example,
   consider NOT:

   - Logical statement: =NOT(A)= or algebraically y = \not x

   - Truth table:
     | A | NOT(A) |
     |---+--------|
     | 0 |      1 |
     | 1 |      0 |

   - Digital circuit diagram:
     #+attr_html: :width 400px :float nil:
     [[../img/Not.png]]

   - Answer:
     #+begin_quote
     Yes: Logical statement, truth table and circuit diagram all show a
     representation of the same Boolean function f:x^{1}{0,1}->{0,1}.

     And No: The level and kind of of abstraction differs: Logic
     notation show algebraic manipulation; truth tables show the state
     space; and circuits show physical realization (voltage, wires,
     gates).

     If the level and kind of abstraction differs, then different
     details have been eliminated and the result has a different
     information content.
     #+end_quote

5. Which of these representations is your personal favorite? Why do
   you think that is?
   #+begin_quote
   - Some properties like identities and laws (De Morgan,
     distribution) are only visible algebraically. Expressions can
     be transformed using these laws. More information for the
     *mathematician* and fan of symbolic manipulation.
   - The truth table completely describes the function for a fixed
     number of inputs, listing all outputs. Its abstraction is close
     to the capabilities of a brute force machine (loop over all
     values). More information for the *computer scientist*.
   - Some properties (timing, propagation delay, power use) are only
     visible in the circuit-level representation: More information
     for the *engineer* and spatial thinker.
   #+end_quote

** IN PROGRESS Overview

- [X] Some questions to begin with
- [ ] Why Computer Architecture
- [ ] Arithmetic
- [ ] Assembly
- [ ] Memory
- [ ] Optimization
- [ ] Networks

** Review (Wednesday)

1. Can you write and run a C++ "hello world" pgm on the command-line?
   #+begin_src bash :results output :exports both
     # write text to stdout and redirect it into a C++ file
     echo -e "#include <iostream>\nint main() {\n  std::cout<<\"hello world\";\n}" > hello.cpp
     cat hello.cpp
     # compile and run C++ file
     g++ hello.cpp -o hello
     ./hello
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main() {
   :   std::cout<<"hello world";
   : }
   : hello world

2. What are the three parts of a computer *system*?
   #+begin_quote
   1) Hardware (CPU),
   2) system software (OS),
   3) application programs (compiler)
   #+end_quote

3. What are the parts of a computer architecture?
   #+begin_quote
   1) ISA (instruction set architecture/CPU),
   2) microarchitecture (data flow/ALU/cache),
   3) system organization (memory bus).
   #+end_quote

** NEXT Assignments for next week

- [X] [[https://lyon.instructure.com/courses/3673/assignments/50009/edit?quiz_lti][Test 1 in Canvas]] (open book) - by September 7 (Friday)

  This first test covers the material seen and taught until Friday,
  August 29. Check the reviews in the agenda.org file to prepare.


- [X] Watch "[[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][How are transistors manufactured?]]" (2024) - 30 min

  This video explains how modern microchips, containing billions of
  nanoscopic (distances of 1 billionth of a meter) transistors, are
  manufactured in semiconductor fabrication plants. Though we're not
  primarily interested in manufacturing chips, this is both
  interesting and relevant to appreciate the complexity of computer
  systems. [Review & Test].

- [X] Watch "[[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Making logic gates from transistors]]" (2015) - 15 min

  The video introduces transistors and shows how they can be used as
  building blocks for digital logic. Though this concerns physics and
  electrical engineering, seeing how logic gates are implemented via
  transistors and circuits will add to your understanding of the
  microarchitecture of computer systems. [Review & Test].

- [X] [[https://lyon.instructure.com/courses/3673/assignments/50014][Install LogiSim on your home desktop or laptop:]]

  1) Install LogiSim on your own computer.
  2) Build a NOT gate as seen in class.
  3) Take a screenshot and upload it to Canvas.

* DONE Week 2: Microchips, logic gates, (Sep 5)

- [X] Popquiz! What do you remember from the videos? Take the
  solutions home, grade yourself, and return the test next week.

- [X] Finish: Assembly and the machine.

** Review: How are Microchips made? ([[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][Branch Education, YouTube 2024]])

1. What is the approximate number of steps required to manufacture a
   modern CPU chip?
   #+begin_quote
   Around 940 steps, taking about 3 months.
   #+end_quote
2. What type of transistor structure is commonly used in today’s CPUs,
   and how small are they?
   #+begin_quote
   FinFET (Fin Field-Effect) transistors, with dimensions on the order
   of tens of nanometers (e.g., 36×6×52 nm). A 3D transistor rather
   than a planar design leading to better control of the current.
   #+end_quote
3. What are the six main categories of fabrication tools used in a
   semiconductor fab?
   #+begin_quote
   Mask-making, deposition, etching/planarization, ion implantation,
   cleaning, and metrology/inspection.

   In the video: Mask layer, adding, removing, modifying, cleaning and
   inspecting  material.
   #+end_quote
4. How does chip “binning” affect the product lines (e.g., i9, i7,
   i5)?
   #+begin_quote
   Chips with defects are categorized based on functional cores and
   features, sold under different product tiers.
   #+end_quote
5. Why is photolithography considered one of the most important steps
   in chip fabrication?
   #+begin_quote
   It transfers nanoscopic circuit patterns from photomasks onto
   wafers, enabling billions of identical transistors and wires.
   #+end_quote

** Summary: How are Microchips made? ([[https://youtu.be/dX9CGRZwD-w?si=KDARgJLQz7Bd3IEu][Branch Education, YouTube 2024]])

The video explains how modern microchips, containing billions of
nanoscopic transistors, are manufactured in semiconductor fabrication
plants:
- Scale & Complexity: A CPU may hold 26 billion transistors across 80
  layers of metal interconnects, manufactured in cleanrooms the size
  of eight football fields, using machines costing up to $170M.
- Transistor Structures: Modern CPUs use FinFET transistors, only tens
  of nanometers in size, smaller than dust particles or mitochondria.
- Manufacturing Analogy: Building a chip is like baking an 80-layer
  cake with 940 steps — requiring precision at every stage or the
  product fails.
- Core Process Steps: Each layer is built by depositing insulators,
  applying light-sensitive photoresist, using photolithography with UV
  light and masks, etching away unwanted areas, depositing copper, and
  leveling the wafer with chemical mechanical planarization
  (CMP). These steps are repeated layer by layer, with frequent
  cleaning and inspection.
- Fabrication Plant Tools: Six categories of tools are used:
  1. Mask-making (photoresist, lithography, stripping)
  2. Deposition (adding metals, oxides, silicon)
  3. Etching & planarization
  4. Ion implantation (doping regions for transistors)
  5. Wafer cleaning
  6. Metrology/inspection
- Throughput & Cost: A fab may hold 435 tools and produce 50,000
  wafers monthly. Each wafer, costing ~$100, becomes worth ~$100,000
  once populated with CPUs.
- Post-Fab Steps: Chips are tested and “binned” (e.g., i9, i7, i5)
  depending on defects, cut from wafers, mounted on packages, fitted
  with heat spreaders, and tested again before sale.
- Broader Context: Microchip fabrication is secretive and
  technologically advanced, requiring immense time and resources. The
  video notes future plans for deep dives on transistors, GPUs, and
  CPU architectures.

** Review: Making logic gates from transistors ([[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Ben Eater, 2015]])

1. What are the three terminals of a transistor and what do they
   represent?
   #+begin_quote
   Emitter, Base, Collector. The base controls current flow between
   collector and emitter.

   #+attr_html: :width 400px :float nil:
   [[../img/transistor.png]]
   #+end_quote
2. What happens in the LED circuit when current flows from the base to
   the emitter?
   #+begin_quote
   The transistor switches on and allows a larger current from collector
   to emitter, lighting the LED.

   #+attr_html: :width 300px :float nil:
   [[../img/led.png]]
   #+end_quote
3. Which logic gate does a single transistor implement when it inverts the input?
   #+begin_quote
   A NOT gate (inverter): input ON → output OFF, input OFF → output ON.

   #+attr_html: :width 300px :float nil:
   [[../img/inverter.png]]

   #+end_quote
4. How is an AND gate built with two transistors?
   #+begin_quote
   The transistors are placed in series. Current flows and the LED turns
   on only if both are conducting (both inputs ON).

   #+attr_html: :width 300px :float nil:
   [[../img/and_circuit2.png]]

   #+end_quote
5. What is the difference between an OR gate and an XOR gate?
   #+begin_quote
   OR: output ON if one or both inputs are ON.  XOR: output ON only if
   exactly one input is ON (off when both are ON).

   #+attr_html: :width 300px :float nil:
   [[../img/or_circuit2.png]]

   #+attr_html: :width 300px :float nil:
   [[../img/xor_circuit2.png]]

   #+end_quote
** Summary: Making logic gates from transistors ([[https://youtu.be/sTu3LwpF6XI?si=k1DRLefz6b9OSKTu][Ben Eater, 2015]])
#+attr_html: :width 600px :float nil:
[[../img/all_gates.png]]

The video introduces transistors and shows how they can be used as
building blocks for digital logic.

- A transistor has three terminals: *emitter*, *base*, *collector*.
- A small base-to-emitter current controls a larger collector-to-emitter current:
  the transistor acts as a *switch*.
- Example 1: Push button + transistor turns an LED on or off.
- Example 2: A transistor can act as an *inverter (NOT gate)*: input ON → LED OFF,
  input OFF → LED ON.
- Combining transistors yields logic gates:
  - *AND gate*: LED on only if both inputs are on.
  - *OR gate*: LED on if either input is on.
  - *XOR gate*: LED on if exactly one input is on (requires five transistors).
- Other gates (NAND, NOR, XNOR, Buffer) can be built by adding inversion.
- With these gates, more complex circuits can be built for arithmetic,
  memory, and eventually entire computers.

** Assembly and the machine

1. Why Assembly at all?
   #+begin_quote
   Assembly is the key to machine-level execution:
   1. Behavior of programs with bugs
   2. Tuning program performance
   3. Implementing system software
   4. Creating/fighting malware
   #+end_quote

2. How can you generate an Assembly file from a C file =hello.c=?
   #+begin_example
   g++ -S hello.c -o hello.s  # output = Assembly
   #+end_example

3. How can you look at the Assembly file?
   #+begin_quote
   With any text editor, or with ~cat~ on the command-line.
   #+end_quote

4. How must you compile to debug your file =segfault.c=?
   #+begin_example
   g++ -g segfault.c -o segfault  # output = Ready for gdb
   #+end_example

5. How can you debug the object code =segfault= with ~gdb~?
   #+begin_example
   gdb segfault  # import segfault and run it in gdb
   #+end_example

* DONE Week 3: Memory, Compiler chain (Sep 8, 10, 12)

- [X] Guest speaker on Wednesday 10 Sept 1 pm in Derby 255
- [X] Memory layout errors
- [X] Code optimization
- [X] Network dependency

- [X] The compiler chain
- [ ] Hardware organization
- [ ] Running the =hello world= executable

- [ ] The OS and its abstractions
- [ ] The memory organization
- [ ] Amdahl's Law
- [ ] Networks
- [ ] Concurrency and parallelism

** Review: Memory (Monday)

1. What's an architecture problem related to memory?
   #+begin_quote
   Memory: Out-of-bounds access can corrupt nearby data because memory
   must be explicitly allocated and managed.
   #+end_quote
2. What's an architecture problem related to performance?
   #+begin_quote
   Performance: Poor memory access patterns (like column-wise instead
   of row-wise) increase cache misses and slow programs.
   #+end_quote
3. What's an architecture problem related to networks?
   #+begin_quote
   Networks: Concurrency, unreliable media, and cross-platform
   differences make network programming complex despite standard
   libraries.
   #+end_quote
4. What's a socket?
   #+begin_quote
   A socket is an endpoint for communication between two programs over
   a network (including communication of a computer with itself). On
   Unix-like systems (Linux, MacOS) it's like a file descriptor - you
   can read from and write to it, but instead of accessing a file, the
   data go through the network stack.
   #+end_quote
5. What's the little/big endian problem?
   #+begin_quote
   The little/big endian problem is about how multi-byte data (like an
   ~int~ or ~double~) is stored in memory:
   - *Little-endian*: the least significant byte goes in the lowest
     memory address.
   - *Big-endian:* the most significant byte goes in the lowest memory
     address.

   The "problem" arises when data is shared between systems with
   different endianness (e.g., over a network or in files). The same
   bytes can be interpreted differently unless both sides agree on the
   byte order.

   In practice, this means that a 32-bit instruction like =0x00c58533=
   on a little endian machine will appear in memory as: =33 85 c5 00=.
   #+end_quote

** Review: Compiler chain (Wednesday)

1. What does ~make~ do? Example use?
   #+begin_quote
   ~make~ works with a configuration ~Makefile~ to compile code. An
   example is =make hello= on the command-line which uses the default
   ~Makefile~ to run =gcc hello.c -o hello= and generate an executable
   =hello= from the source file =hello.c=.
   #+end_quote

2. What is shipped alongside software source code to enable portable
   object code?
   #+begin_quote
   A build system or ~Makefile~. The compiler uses the ~Makefile~ to build
   the software for the given computer architecture.

   Portable object code = source code + Makefile/configuration.
   #+end_quote

3. Why does each character in =hello.c= have an associated number like
   104 for =h=? What does this have to do with bytes?
   #+begin_quote
   Because text is stored as bytes using ASCII encoding: each
   character is mapped to a unique integer between 0 and 255
   (2^8-1). For example, the letter =A= is ASCII =65= which in binary is
   =01000001= (2^6+1). One ASCII character = 1 byte (8 bits) in memory.
   #+end_quote

4. What does the preprocessor do in the compilation chain?
   #+begin_quote
   The preprocessor expands macros (like =#define PI 3.14=) and includes
   header files (like =#include <stdio.h>=) to create an intermediate
   (=.i=) source file.
   #+end_quote

5. In the compilation chain, what is the role of the assembler?
   #+begin_quote
   The assembler translates assembly code (=.s=) into object code
   (=.o=). With ~gdb~, you can disassemble the object machine code to see
   the assembly (as text =.s=).
   #+end_quote

   #+attr_html: :width 00px :float nil:
   #+caption: Compilation chain. Source: Bryant/O'Halloran 2016 (Fig 1.3)
   [[../img/fig1.3_compilation.png]]

* DONE Week 4: Expo, Hardware Organization, Operating System (Sep 15, 17, 19)
#+attr_html: :width 200px :float nil:
[[../img/expo.png]]

*Housekeeping:*

- [X] I made the first assignment (memory out-of-bounds demonstration)
  a little more verbose and a little simpler. If you didn't submit on
  time or you don't understand it: Ask another student, submit late
  for 50%.

- [X] Test 3 is live (25 questions).

- [ ] Two assignments are live - compiler chain and
  makefile. Unfortunately, Canvas allows only upload of one file or
  text. If you cannot fit the output on a screenshot, you need to
  submit a ZIP file.

- [X] For the ~Makefile~ assignment, just submit the results as text -
  copy of the Makefile and copy of the command-line dialogue. Like this:
  #+attr_html: :width 400px:
  [[../img/makefile.png]]

- [X] You should go to the Career Fair tomorrow. Will you?

- [X] What did you think of the Career Fair?

- [ ] Popquiz 2 (self-graded): Computer hardware organization

- [ ] Take the solution home with you. You can hand in your solution
  for bonus points if you like.

- [ ] Test 4 coming on the weekend.

*Content:*

- [X] Hardware organization
- [ ] Operating system and hardware
- [ ] Networks and hardware
- [ ] Amdahl's Law (a law about systems)
- [ ] Concurrency and parallelism

** Review: Compiler chain / Makefile

1. What kind of file does the compiler (~cc1~) produce?
   #+begin_quote
   Assembly source code (.s) for the target machine architecture.
   #+end_quote

2. What is the role of the assembler (~as~)?
   #+begin_quote
   It translates assembly code (.s) into an object file (.o) containing
   machine code instructions.
   #+end_quote

3. What does the linker (~ld~) do?
   #+begin_quote
   It combines object files (.o) and library code into a final
   executable file.
   #+end_quote

4. What is the purpose of a Makefile?
   #+begin_quote
   It automates the build process by defining rules, dependencies, and
   commands to compile programs efficiently.
   #+end_quote

5. What happens if you run =make hello= and =hello.o= is already up to
   date (i.e. the timestamp of =hello.o= is more recent than =hello.c=)?
   #+begin_quote
   Nothing is rebuilt, because ~make~ sees that the target is newer than
   its dependencies.
   #+end_quote

6. Does ~make~ only work on C programs, or also on C++ programs? And
   what about Python or R? What about Java?
   #+begin_src bash :results output
     echo -e "#include <iostream>\nint main(){std::cout<<\"hello\";return 0;}" > hello.cpp
     cat hello.cpp
     make hello  # uses the default Makefile
   #+end_src

   #+RESULTS:
   : #include <iostream>
   : int main(){std::cout<<"hello";return 0;}
   : g++     hello.cpp   -o hello

   #+begin_src bash :results output
     echo -e "str(mtcars)" > mtcars.R
     cat mtcars.R
     make mtcars
     Rscript ./mtcars.R
   #+end_src

   #+RESULTS:
   #+begin_example
   str(mtcars)
   'data.frame':        32 obs. of  11 variables:
    $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
    $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
    $ disp: num  160 160 108 258 360 ...
    $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
    $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
    $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
    $ qsec: num  16.5 17 18.6 19.4 17 ...
    $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
    $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
    $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
    $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
   #+end_example

   #+begin_src bash :results output
     echo -e "print(\"hello\")" > hellopy.py
     cat hellopy.py
     make hellopy
     #python3 hellopy.py
   #+end_src

   #+RESULTS:
   : print("hello")

   #+begin_src bash :results output
     echo -e "System.out.println(\"hello\");" > helloJava.java
     cat helloJava.java
     make helloJava
   #+end_src

   #+RESULTS:
   : System.out.println("hello");

7. How must commands in a Makefile begin?
   #+begin_quote
   The =make= utility expects commands in each rule to be indented with
   a TAB character, not spaces — otherwise it throws an error.
   #+end_quote

8. How does =make clean= typically work?
   #+begin_quote
   It removes intermediate files (like .i, .s, .o, executables) so you
   can rebuild from scratch.
   #+end_quote

9. Which devices are involved in running a simple program like =hello=?
   #+begin_quote
   Execution requires cooperation between CPU, memory, input/output
   system, graphics/display system, and keyboard for interaction.
   #+end_quote

10. What is a =checksum=?
    #+begin_quote
    A *checksum* is a fixed-size value that is sent/store alongside with
    data, for example precompiled software. At the receiving end, the
    checksum is recomputed and compared with the sent/stored one. If
    the values don't match, the data was likely corrupted or tampered
    with.
    #+end_quote

11. What do the commands =echo $PS1= and =echo $SHELL= mean and return
    (only on Linux)?
    #+begin_example sh
      ## shell prompt definition string variable
      echo $PS1     # on my laptop: \u@marcus@dell:\w $
      ## shell environment variable
      echo $SHELL   # /bin/bash
    #+end_example

** Review: Hardware Organization

1. What is the main role of a *bus* in computer hardware organization?
   #+begin_quote
   A bus acts as a bridge that carries data back and forth between
   system components, typically designed for byte transfers (8-bit
   chunks on a 64-bit system).
   #+end_quote

2. How are *controllers* and *adapters* different in connecting devices to
   the I/O bus?
   #+begin_quote
   A controller is a *chip* located directly *on the device* (e.g., USB
   controller on keyboard), while an adapter is a separate *card* that
   *plugs* into the motherboard (e.g., graphics adapter, WiFi adapter).
   #+end_quote

3. What are the four basic CPU transactions, and what do they accomplish?
   #+begin_quote
   1. Load: copy a byte from main memory into a register.
   2. Store: copy a byte from a register into main memory.
   3. Operate: perform arithmetic/logic using ALU and registers.
   4. Jump: copy a value into the program counter to change execution
      flow.
   #+end_quote

4. What role does CMOS play in a computer system?
   #+begin_quote
   CMOS stores system configuration data (such as BIOS settings, date,
   and time) using a small battery so that the information persists
   even when the computer is powered off.
   #+end_quote

5. Why is main memory (RAM) considered temporary storage, and what
   does it physically consist of?
   #+begin_quote
   RAM is temporary storage because its contents (code and data) are
   lost when power is off. Physically, it consists of DRAM chips.
   #+end_quote

* Week 5: Virtual memory, Kernel probe (Sep 22, 24, 26)
#+attr_html: :width 600px:
[[../img/virtual_memory.png]]

** DONE Review (Friday)

1. What is the main principle of the memory hierarchy?
   #+begin_quote
   Place small, fast storage near the CPU, and larger, slower storage
   far away. The smallest, fastest are called "cache memory".
   #+end_quote

2. Why do SSDs wear out over time? How many writes does a cell have?
   #+begin_quote
   Electron tunneling damages the thin oxide layer in memory cells. A
   cell allows about 100,000 writes (more with error correction).
   #+end_quote

3. Number of NAND memory cells in a 4GB RAM module, approximately?
   #+begin_quote
   Approximately 34 billion densely packed cells: 4 x 2^30 x 8
   #+end_quote

4. What is the OS kernel?
   #+begin_quote
   The OS part that is always in memory, loaded first, controls the
   system resources with interrupt calls (=signal(7)=)
   #+end_quote

5. Which abstractions does the OS provide to applications?
   #+begin_quote
   Processes, virtual memory, file system controlled by the kernel.
   #+end_quote

** DONE Review (Monday)

1. What is the key difference between a thread and a process in terms
   of memory usage?
   #+begin_quote
   Threads share the same code and global data, but each has its own
   stack and registers. Processes have separate memory spaces.
   #+end_quote

2. When are shared libraries (~.so~) linked to the executable?
   #+begin_quote
   Shared libraries (~.so~) are not copied into the executable at
   compile time.  They are dynamically linked at run-time by the
   dynamic linker/loader ~ldd~ when the program starts. This allows
   smaller executables and library updates without recompilation.
   #+end_quote

3. Why is output from multiple C++ threads using ~std::cout~ potentially
   unsafe without a =mutex= ("mutual exclusion", as shown in class)?
   #+begin_quote
   Because the ~ostream~ buffer is not protected, outputs from different
   process threads can become interleaved, confusing the computer.
   #+end_quote

4. What is virtual memory?
   #+begin_quote
   Virtual memory is the abstraction layer that every process
   sees. Inside that address space, the OS arranges different regions
   for program use:
   - =.text= segment – executable code (fixed, read-only).
   - =.data= segment – global and static variables (read/write).
   - Heap – dynamically allocated memory (malloc, new), grows upward.
   - Stack – function call frames and local variables, grows downward.
   #+end_quote

5. What is the big memory illusion of the OS?
   #+begin_quote
   Virtual memory maps each process’s addresses to physical memory, so
   every process sees the same private address space even though
   memory is shared.
   #+end_quote

6. In the C++ memory example, where are the following stored:
   1) =global_var=
   2) =local_var=, =local_var2=
   3) =heap_var=, =heap_var2=
   #+begin_src C++ :main no :includes :results output :exports both
     #include <iostream>
     using namespace std;

     int global_var = 42;

     int main(void)
     {
       int local_var = 1;   // Stack
       int local_var2 = 2;  // Stack
       int* heap_var = new int(99);  // heap
       int* heap_var2 = new int(98); // heap

       cout << "local variables:\n" << &local_var   << endl
            << &local_var2  << endl
            << "heap variables:\n"    << heap_var   << endl
            << heap_var     << endl
            << "global variable:\n" << &global_var  << endl;

       delete heap_var;
       delete heap_var2;
       return 0;
     }
   #+end_src

   #+RESULTS:
   : local variables:
   : 0x7ffee9dc3e50
   : 0x7ffee9dc3e54
   : heap variables:
   : 0x559bfad0aeb0
   : 0x559bfad0aeb0
   : global variable:
   : 0x559bceb14010

   #+begin_quote
   1) ~.data~ segment (global storage)
   2) Stack (local variables disappear when their scope vanishes)
   3) Heap (multiple small =new= calls may come from the same memory)
   #+end_quote

7. If the stack grows downward and the heap grows upward, why don't
   you see this in the previous example?
   #+begin_quote
   OS randomize memory region start addresses for security - this is
   called "Address Space Layout Randomization" (ASLR). Small ~new~
   allocations on the heap may reuse the same memory.
   #+end_quote

   #+begin_src C++ :main no :includes :results output :exports both
     #include <iostream>
     using namespace std;

     int global_var = 42; // in .data segment

     // Recursive function to show stack growth
     void stack_demo(int depth) {
       int local_var = depth;
       cout << "stack depth " << depth
            << " local_var address: " << &local_var << endl;

       if (depth < 5) stack_demo(depth + 1);
     }

     int main() {
       cout << "global variable address: " << &global_var << endl;

       // Observe stack growth
       cout << "\n--- Stack growth ---" << endl;
       stack_demo(1);

       // Observe heap growth
       cout << "\n--- Heap growth ---" << endl;
       int* heap_vars[5];
       for (int i = 0; i < 5; i++) {
         heap_vars[i] = new int(i);
         cout << "heap allocation " << i
         << " address: " << heap_vars[i] << endl;
       }

       // Clean up heap memory
       for (int i = 0; i < 5; i++) {
         delete heap_vars[i];
       }

       return 0;
     }
   #+end_src

   #+RESULTS:
   #+begin_example
   global variable address: 0x562bb9adb010

   --- Stack growth ---
   stack depth 1 local_var address: 0x7ffe6c404a64
   stack depth 2 local_var address: 0x7ffe6c404a34
   stack depth 3 local_var address: 0x7ffe6c404a04
   stack depth 4 local_var address: 0x7ffe6c4049d4
   stack depth 5 local_var address: 0x7ffe6c4049a4

   --- Heap growth ---
   heap allocation 0 address: 0x562bbcdb8ec0
   heap allocation 1 address: 0x562bbcdb8ee0
   heap allocation 2 address: 0x562bbcdb8f00
   heap allocation 3 address: 0x562bbcdb8f20
   heap allocation 4 address: 0x562bbcdb8f40
   #+end_example

   #+begin_quote
   - Stack stride: Each step subtracts 0x30 = 48 (3 * 16^1) from the
     previous address (compiler dependent).
   - Heap stride: Each step adds 32 (2 * 16) (allocator alignment).
   #+end_quote

   #+begin_src C++ :main yes :includes <iostream> <iomanip> :results output :exports both :comments both :tangle yes :noweb yes
     std::cout << 0x7ffd7f8c7384 - 0x7ffd7f8c7354; // stack grows down
     std::cout << std::endl;
     std::cout << 0x5591631c8f40 - 0x5591631c8f20; // heap grows up
   #+end_src

   #+RESULTS:
   : 48
   : 32

8. Where are shared libraries (~.so~) held in memory?
   #+begin_quote
   Shared libraries are mapped into a process’s virtual address space
   in a special region, separate from the program’s own code, data,
   stack, and heap.

   They are loaded into memory once by the dynamic linker and can be
   shared across processes (read-only parts like code), while each
   process gets its own writable copy of data sections.
   #+end_quote

9. Are there also static libraries? What's the difference?
   #+begin_quote
   - A static library (~.a~) is linked at compile-time. Its code is
     copied directly into the executable, so no external file is
     needed at run time. Example: =libm.a= (math functions like =sqrt=).

   - A shared library (~.so~) is linked at run-time, and its code is
     loaded into memory by the dynamic linker. Multiple processes can
     share the same library code in RAM. Example: =libc.so=
     (e.g. =printf=, =pthread=, =malloc=, =free=).
   #+end_quote

10. What is ~systemd~?
    #+begin_quote
    ~systemd~ is the central manager that initializes and controls
    services and resources on a Linux system after the kernel has
    booted (process PID = 1).
    #+end_quote

11. What's the difference between these declarations with regard to
    memory?
    #+begin_example C++
      vec[] {1,2,3};   // array definition (stack)
      vector<int> vec2; // vector definition (heap)
    #+end_example

    #+begin_quote
    - =vec[] {1,2,3};= is a statically allocated object at compile-time.
    - =vector<int> vec2;= is a dynamically allocated object at run-time.
    #+end_quote

12. What do "concurrency" and "parallelism" refer to?
    #+begin_quote
    - *Concurrency* means structuring a program so that multiple tasks
      can make progress during the same period of time usually via
      threads that share code and memory.
    - *Parallelism* means tasks are executed at the same physical time
      on multiple CPU cores or processors with separate code and
      memory.
    #+end_quote

** NEXT Review (Wednesday) - Program Memory Layout

1. What memory segments hold program code and global initialized data?
   #+begin_quote
   =.text= (code) and =.data= (global initialized variables).
   #+end_quote

2. How does the heap expand and contract during program execution?
   #+begin_quote
   Via dynamic allocation calls (~malloc/free~ in C, ~new/delete~ in C++).
   #+end_quote

3. What is the role of the dynamic linker (~ld.so~) with shared
   libraries?
   #+begin_quote
   It maps *.so* files into the program’s memory at runtime.
   #+end_quote

4. Where is the user stack located in the virtual address space, and
   how does it behave?
   #+begin_quote
   At the *top* of the address space; it grows and shrinks with function
   calls.
   #+end_quote

5. In C++, where do ~std::vector<int>~ objects and their contents
   reside?
   #+begin_quote
   The object lives on the *stack*, but its dynamic array is stored on
   the *heap*.
   #+end_quote

6. Why do program code and global data always start at the same memory
   address (=0x0=)?
   #+begin_quote
   To ensure predictable linking and loading of executables.
   #+end_quote

7. What happens when you call =new T= in C++?
   #+begin_quote
   It constructs an object of type *T* and allocates memory for it on
   the heap.
   #+end_quote

8. Give an example of two common shared libraries on Linux.
   #+begin_quote
   ~libm.so~ (math functions) and ~libc.so~ (standard C library).
   #+end_quote

9. What tool can show which shared libraries an executable will load
   at runtime?
   #+begin_quote
   The `ldd` command.
   #+end_quote
   #+begin_src C :tangle ../src/helloldd.c :main yes :includes <stdio.h> <stdlib.h> <string.h> :results output :exports both :noweb yes
     printf("Hello");
   #+end_src

   #+RESULTS:
   : Hello

   #+begin_src bash :results output :exports both
     cd ../src
     make helloldd
     ldd helloldd
   #+end_src

   #+RESULTS:
   : make: 'helloldd' is up to date.
   :    linux-vdso.so.1 (0x0000736e15556000)
   :    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x0000736e15200000)
   :    /lib64/ld-linux-x86-64.so.2 (0x0000736e15558000)

10. How does =vector.push_back= affect memory allocation?
    #+begin_quote
    It may reallocate the internal array on the heap, but stack
    metadata is unchanged.
    #+end_quote

11. Why is the =.text= segment immutable at runtime?
    #+begin_quote
    To prevent accidental or malicious modification of program code,
    ensuring safety and stability.
    #+end_quote

12. Compare how memory is released on the stack vs. the heap.
    #+begin_quote
    Stack memory is released automatically when functions return; heap
    memory must be freed manually (~free~ / ~delete~).
    #+end_quote

13. Why is dynamic linking with shared libraries more efficient than
    static linking?
    #+begin_quote
    It avoids code duplication across programs and reduces executable
    size by loading one shared copy at runtime.
    #+end_quote

14. How does the growth direction of the stack differ from the heap?
    #+begin_quote
    The stack typically grows *downward* (toward lower addresses), while
    the heap grows *upward* (toward higher addresses).
    #+end_quote

15. Why does ~std::vector~ use the heap for its contents instead of the
    stack?
    #+begin_quote
    Because its size is determined at runtime, requiring flexible
    memory that the stack cannot provide.
    #+end_quote

** PRACTICE Memory layout exploration (ca. 30-40 min)

- [ ] Open ide.cloud.google.com and go to the terminal.
- [ ] Code along with me. Answer the questions.
- [ ] Upload files (=mem.c=, =stack.c= and =Makefile=) as .zip to Canvas.

* Week 6: Amdahl's Law, Networks, Parallelism (Sep 29, Oct 1, Oct 3)

** Amdahl's Law

#+attr_html: :width 300px:
[[../img/amdahl.png]]
#+begin_quote
Figure: Amdahl's Law demonstrates the theoretical maximum speedup of
an overall system and the concept of diminishing returns. Plotted here
is logarithmic parallelization vs linear speedup. If exactly 50% of
the work can be parallelized, the best possible speedup is 2 times. If
95% of the work can be parallelized, the best possible speedup is 20
times. According to the law, even with an infinite number of
processors, the speedup is constrained by the unparallelizable
portion.
#+end_quote

- [ ] Test 4 is live (Sep 28).
- [ ] Assignment: Pushing a module to the kernel (Sep 28).
- [ ] Video (Google Chat): OS kernel and computer architecture.
